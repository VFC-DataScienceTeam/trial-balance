{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c060e73e",
   "metadata": {
    "papermill": {
     "duration": 0.018092,
     "end_time": "2025-11-21T07:52:34.622717",
     "exception": false,
     "start_time": "2025-11-21T07:52:34.604625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trial Balance Automation - MVP\n",
    "\n",
    "**Purpose**: Load, validate, and analyze trial balance data\n",
    "\n",
    "**Author**: Raiden Velarde Guillergan - Data Scientist \n",
    "\n",
    "**Date**: November 4, 2025\n",
    "\n",
    "**Data Source**: `data/raw/Trial Balance/2025/September/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70139a03",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Workflow Diagram\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    Start([Start]) --> Init[1. Initialize<br/>Libraries & Logger]\n",
    "    Init --> LoadFunc[2-3. Define<br/>Loading Functions]\n",
    "    LoadFunc --> Load[4. Load Data<br/>TB + References]\n",
    "    Load --> Separate[5. Separate Data]\n",
    "    Separate --> AddDate[6. Add Date Column]\n",
    "    AddDate --> Consolidate[7. Consolidate TB]\n",
    "    Consolidate --> Pivot[8. Create Pivot Table]\n",
    "    Pivot --> Match[9. Match GL Accounts]\n",
    "    Match --> CheckNew{New Accounts?}\n",
    "    CheckNew -->|Yes| Export[Export Updated COA]\n",
    "    CheckNew -->|No| Done\n",
    "    Export --> Done([End])\n",
    "    \n",
    "    style Start fill:#e1f5e1\n",
    "    style Done fill:#ffe1e1\n",
    "    style Pivot fill:#f0e1ff\n",
    "    style Export fill:#e1f0ff\n",
    "```\n",
    "\n",
    "**Note**: Install `Markdown Preview Mermaid Support` extension to view diagrams.  \n",
    "**Full Documentation**: See `docs/workflow-diagram.md`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a31534",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11919e66",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from typing import Union, List, Dict, Any, Hashable, Tuple\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import numbers, Alignment, Font # Import for formatting, alignment, and bolding\n",
    "from openpyxl.utils import get_column_letter # Import for column letter lookup, needed for merging\n",
    "\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac7a51",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bbed9b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup logging configuration\n",
    "import logging\n",
    "\n",
    "# Get project root for absolute paths\n",
    "try:\n",
    "    if '__file__' in globals():\n",
    "        notebook_dir = Path(__file__).parent\n",
    "    else:\n",
    "        notebook_dir = Path.cwd()\n",
    "        if notebook_dir.name != 'notebooks':\n",
    "            notebook_dir = notebook_dir / 'notebooks'\n",
    "    project_root = notebook_dir.parent if notebook_dir.name == 'notebooks' else notebook_dir\n",
    "except:\n",
    "    project_root = Path.cwd()\n",
    "    if project_root.name == 'notebooks':\n",
    "        project_root = project_root.parent\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "log_dir = project_root / 'logs'\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create log filename with timestamp\n",
    "log_filename = f\"trial_balance_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "log_path = log_dir / log_filename\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_path),\n",
    "        logging.StreamHandler()  # Also print to console\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"TRIAL BALANCE AUTOMATION - LOGGING INITIALIZED\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(f\"Project Root: {project_root}\")\n",
    "logger.info(f\"Log file: {log_path}\")\n",
    "logger.info(f\"Session started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "logger.info(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "print(f\"\\nâœ“ Logging configured successfully\")\n",
    "print(f\"ðŸ“‚ Project Root: {project_root}\")\n",
    "print(f\"ðŸ“ Log file: {log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53db6e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Parameters (for Papermill)\n",
    "\n",
    "These parameters can be overridden when running via papermill/GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed52229",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters - can be overridden by papermill\n",
    "# Tag this cell with 'parameters' for papermill to inject values\n",
    "# NOTE: These are NOT used anymore - config file approach is used instead\n",
    "year = None  # Deprecated - use config file\n",
    "month = None  # Deprecated - use config file\n",
    "\n",
    "logger.info(\"ðŸ“ Parameters cell (deprecated - using config file approach instead)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc75be",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c99fc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 2. Data Loading Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08828cf3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 3. Reference Data Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43431b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_reference_data(base_path=None):\n",
    "    \"\"\"\n",
    "    Load reference data (COA Mapping and Portfolio Mapping) from the latest files.\n",
    "    Supports both CSV and XLSX file formats.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Optional path to references folder. If None, uses project_root/data/references\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing:\n",
    "            - 'coa_mapping': DataFrame from COA Mapping folder (latest file)\n",
    "            - 'portfolio_mapping': DataFrame from Portfolio Mapping folder (latest file)\n",
    "            - 'metadata': dict with loading information\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use absolute path from project root if not provided\n",
    "    if base_path is None:\n",
    "        base_path = project_root / 'data' / 'references'\n",
    "    else:\n",
    "        base_path = Path(base_path)\n",
    "    \n",
    "    # Initialize result dictionary\n",
    "    result = {\n",
    "        'coa_mapping': None,\n",
    "        'portfolio_mapping': None,\n",
    "        'metadata': {\n",
    "            'load_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'coa_mapping_file': None,\n",
    "            'portfolio_mapping_file': None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Helper function to load file (CSV or XLSX)\n",
    "    def load_file(file_path):\n",
    "        if file_path.suffix.lower() == '.csv':\n",
    "            return pd.read_csv(file_path)\n",
    "        elif file_path.suffix.lower() in ['.xlsx', '.xls']:\n",
    "            return pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {file_path.suffix}\")\n",
    "    \n",
    "    # Define folder paths\n",
    "    coa_mapping_folder = base_path / 'COA Mapping'\n",
    "    portfolio_mapping_folder = base_path / 'Portfolio Mapping'\n",
    "    \n",
    "    # ========== Load COA Mapping (Latest File) ==========\n",
    "    if coa_mapping_folder.exists():\n",
    "        logger.info(f\"ðŸ“‚ Loading COA Mapping from: {coa_mapping_folder}\")\n",
    "        \n",
    "        # Get all CSV and XLSX files sorted by modification time (latest first)\n",
    "        files = sorted(\n",
    "            list(coa_mapping_folder.glob('*.csv')) + \n",
    "            list(coa_mapping_folder.glob('*.xlsx')) + \n",
    "            list(coa_mapping_folder.glob('*.xls')),\n",
    "            key=lambda f: f.stat().st_mtime, \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        if not files:\n",
    "            logger.warning(f\"  âš ï¸  WARNING: No CSV or XLSX files found in {coa_mapping_folder}\")\n",
    "        else:\n",
    "            latest_file = files[0]\n",
    "            result['coa_mapping'] = load_file(latest_file)\n",
    "            result['metadata']['coa_mapping_file'] = latest_file.name\n",
    "            \n",
    "            logger.info(f\"  âœ“ Loaded latest file: {latest_file.name}\")\n",
    "            logger.info(f\"    Records: {len(result['coa_mapping'])}\")\n",
    "            \n",
    "            if len(files) > 1:\n",
    "                logger.info(f\"    Note: {len(files)} files found, loaded the most recent\")\n",
    "    else:\n",
    "        logger.warning(f\"âš ï¸  WARNING: COA Mapping folder not found: {coa_mapping_folder}\")\n",
    "    \n",
    "    # ========== Load Portfolio Mapping (Latest File) ==========\n",
    "    if portfolio_mapping_folder.exists():\n",
    "        logger.info(f\"\\nðŸ“‚ Loading Portfolio Mapping from: {portfolio_mapping_folder}\")\n",
    "        \n",
    "        # Get all CSV and XLSX files sorted by modification time (latest first)\n",
    "        files = sorted(\n",
    "            list(portfolio_mapping_folder.glob('*.csv')) + \n",
    "            list(portfolio_mapping_folder.glob('*.xlsx')) + \n",
    "            list(portfolio_mapping_folder.glob('*.xls')),\n",
    "            key=lambda f: f.stat().st_mtime, \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        if not files:\n",
    "            logger.warning(f\"  âš ï¸  WARNING: No CSV or XLSX files found in {portfolio_mapping_folder}\")\n",
    "        else:\n",
    "            latest_file = files[0]\n",
    "            result['portfolio_mapping'] = load_file(latest_file)\n",
    "            result['metadata']['portfolio_mapping_file'] = latest_file.name\n",
    "            \n",
    "            logger.info(f\"  âœ“ Loaded latest file: {latest_file.name}\")\n",
    "            logger.info(f\"    Records: {len(result['portfolio_mapping'])}\")\n",
    "            \n",
    "            if len(files) > 1:\n",
    "                logger.info(f\"    Note: {len(files)} files found, loaded the most recent\")\n",
    "    else:\n",
    "        logger.warning(f\"âš ï¸  WARNING: Portfolio Mapping folder not found: {portfolio_mapping_folder}\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc8a29",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Data with Config + Fallback\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(\"LOADING TRIAL BALANCE DATA\")\n",
    "logger.info(\"=\"*60)\n",
    "\n",
    "# Define paths - Get absolute path to project root first\n",
    "# When running via papermill, __file__ is available\n",
    "# When running in Jupyter, use Path.cwd() and navigate up\n",
    "try:\n",
    "    # Try to get notebook's actual location\n",
    "    if '__file__' in globals():\n",
    "        notebook_dir = Path(__file__).parent\n",
    "    else:\n",
    "        # In Jupyter/interactive mode, assume we're in notebooks/\n",
    "        notebook_dir = Path.cwd()\n",
    "        if notebook_dir.name != 'notebooks':\n",
    "            notebook_dir = notebook_dir / 'notebooks'\n",
    "    \n",
    "    # Project root is parent of notebooks directory\n",
    "    if notebook_dir.name == 'notebooks':\n",
    "        project_root = notebook_dir.parent\n",
    "    else:\n",
    "        project_root = notebook_dir\n",
    "        \n",
    "except:\n",
    "    # Fallback: use current directory\n",
    "    project_root = Path.cwd()\n",
    "    if project_root.name == 'notebooks':\n",
    "        project_root = project_root.parent\n",
    "\n",
    "# Construct absolute paths\n",
    "config_path = project_root / 'config' / 'run_config.json'\n",
    "default_raw_path = project_root / 'data' / 'raw' / 'Trial Balance'\n",
    "\n",
    "logger.info(f\"ðŸ“‚ Project Root: {project_root}\")\n",
    "logger.info(f\"ðŸ“‚ Config Path: {config_path}\")\n",
    "logger.info(f\"ðŸ“‚ Default Raw Path: {default_raw_path}\")\n",
    "\n",
    "# Initialize variables\n",
    "data_path = None\n",
    "year = None\n",
    "month = None\n",
    "config_source = None\n",
    "\n",
    "# ========== STRATEGY 1: Try Config File (User Selection) ==========\n",
    "if config_path.exists():\n",
    "    logger.info(f\"ðŸ“ Found config file: {config_path}\")\n",
    "    \n",
    "    try:\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        data_path = Path(config.get('data_path', ''))\n",
    "        year = config.get('year')\n",
    "        month = config.get('month')\n",
    "        \n",
    "        logger.info(f\"âœ… Config loaded successfully\")\n",
    "        logger.info(f\"   Year: {year}\")\n",
    "        logger.info(f\"   Month: {month}\")\n",
    "        logger.info(f\"   Data Path: {data_path}\")\n",
    "        \n",
    "        # Validate the path exists\n",
    "        if data_path.exists():\n",
    "            config_source = \"user_selection\"\n",
    "            logger.info(f\"âœ… Data folder validated: {data_path}\")\n",
    "        else:\n",
    "            logger.warning(f\"âš ï¸  Config path does not exist: {data_path}\")\n",
    "            logger.warning(f\"   Will try fallback methods...\")\n",
    "            data_path = None\n",
    "            \n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"âŒ Config file is corrupted: {e}\")\n",
    "        logger.warning(f\"   Will try fallback methods...\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"âŒ Error reading config: {e}\")\n",
    "        logger.warning(f\"   Will try fallback methods...\")\n",
    "else:\n",
    "    logger.warning(f\"âš ï¸  Config file not found: {config_path}\")\n",
    "    logger.info(f\"   This is normal for direct notebook execution\")\n",
    "\n",
    "# ========== STRATEGY 2: Auto-Detect Latest (Fallback) ==========\n",
    "if data_path is None:\n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"FALLBACK: AUTO-DETECTING LATEST DATA FOLDER\")\n",
    "    logger.info(\"=\"*60)\n",
    "    \n",
    "    if not default_raw_path.exists():\n",
    "        error_msg = f\"âŒ FATAL: Raw data folder not found: {default_raw_path}\"\n",
    "        logger.error(error_msg)\n",
    "        logger.error(f\"   Project Root: {project_root}\")\n",
    "        logger.error(f\"   Please check your project structure!\")\n",
    "        logger.error(f\"   Current working directory: {Path.cwd()}\")\n",
    "        raise FileNotFoundError(error_msg)\n",
    "    \n",
    "    # Find latest year (only numeric folders)\n",
    "    year_folders = sorted(\n",
    "        [f for f in default_raw_path.iterdir() if f.is_dir() and f.name.isdigit()],\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    if not year_folders:\n",
    "        error_msg = f\"âŒ FATAL: No year folders found in {default_raw_path}\"\n",
    "        logger.error(error_msg)\n",
    "        logger.error(f\"   Available folders: {[f.name for f in default_raw_path.iterdir() if f.is_dir()]}\")\n",
    "        raise FileNotFoundError(error_msg)\n",
    "    \n",
    "    latest_year_folder = year_folders[0]\n",
    "    year = latest_year_folder.name\n",
    "    logger.info(f\"ðŸ“… Latest year found: {year}\")\n",
    "    \n",
    "    # Find latest month (by modification time)\n",
    "    month_folders = sorted(\n",
    "        [f for f in latest_year_folder.iterdir() if f.is_dir()],\n",
    "        key=lambda x: x.stat().st_mtime,\n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    if not month_folders:\n",
    "        error_msg = f\"âŒ FATAL: No month folders found in {latest_year_folder}\"\n",
    "        logger.error(error_msg)\n",
    "        raise FileNotFoundError(error_msg)\n",
    "    \n",
    "    latest_month_folder = month_folders[0]\n",
    "    month = latest_month_folder.name\n",
    "    data_path = latest_month_folder\n",
    "    config_source = \"auto_detect\"\n",
    "    \n",
    "    logger.info(f\"ðŸ“… Latest month found: {month}\")\n",
    "    logger.info(f\"âœ… Auto-detected path: {data_path}\")\n",
    "\n",
    "# ========== FINAL VALIDATION ==========\n",
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"DATA LOADING SUMMARY\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(f\"Source: {config_source.upper().replace('_', ' ')}\")\n",
    "logger.info(f\"Year: {year}\")\n",
    "logger.info(f\"Month: {month}\")\n",
    "logger.info(f\"Data Path: {data_path}\")\n",
    "\n",
    "# Validate required subfolders exist\n",
    "tb_folder = data_path / 'Trial Balance'\n",
    "coa_folder = data_path / 'Chart of Accounts'\n",
    "\n",
    "if not tb_folder.exists():\n",
    "    logger.error(f\"âŒ Trial Balance folder missing: {tb_folder}\")\n",
    "    raise FileNotFoundError(f\"Required folder not found: {tb_folder}\")\n",
    "    \n",
    "if not coa_folder.exists():\n",
    "    logger.warning(f\"âš ï¸  Chart of Accounts folder missing: {coa_folder}\")\n",
    "\n",
    "# ========== LOAD DATA ==========\n",
    "try:\n",
    "    # Load trial balance data using the determined path\n",
    "    data = {\n",
    "        'trial_balance': {},\n",
    "        'chart_of_accounts': None,\n",
    "        'metadata': {\n",
    "            'year': year,\n",
    "            'month': month,\n",
    "            'load_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'tb_files': []\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Load Trial Balance CSVs\n",
    "    logger.info(f\"\\nðŸ“Š Loading Trial Balance files from: {tb_folder}\")\n",
    "    for file in sorted(tb_folder.glob(\"*.csv\")):\n",
    "        try:\n",
    "            file_date = datetime.strptime(file.stem, \"%m-%d-%Y\")\n",
    "            date_key = file_date.strftime(\"%Y-%m-%d\")\n",
    "            data['trial_balance'][date_key] = pd.read_csv(file)\n",
    "            data['metadata']['tb_files'].append({\n",
    "                'filename': file.name,\n",
    "                'date': date_key,\n",
    "                'records': len(data['trial_balance'][date_key])\n",
    "            })\n",
    "            logger.info(f\"  âœ… {file.name}: {len(data['trial_balance'][date_key])} records\")\n",
    "        except ValueError:\n",
    "            logger.warning(f\"  âš ï¸  Skipped non-standard filename: {file.name}\")\n",
    "    \n",
    "    # Load Chart of Accounts\n",
    "    coa_files = list(coa_folder.glob(\"*.csv\"))\n",
    "    if coa_files:\n",
    "        logger.info(f\"\\nðŸ“‹ Loading Chart of Accounts from: {coa_folder}\")\n",
    "        data['chart_of_accounts'] = pd.read_csv(coa_files[0])\n",
    "        logger.info(f\"  âœ… {coa_files[0].name}: {len(data['chart_of_accounts'])} records\")\n",
    "    \n",
    "    logger.info(\"\\n\" + \"=\"*60)\n",
    "    logger.info(\"âœ… DATA LOADED SUCCESSFULLY\")\n",
    "    logger.info(\"=\"*60)\n",
    "    logger.info(f\"Trial Balance Files: {len(data['trial_balance'])}\")\n",
    "    logger.info(f\"Chart of Accounts: {'Loaded' if data['chart_of_accounts'] is not None else 'Not Loaded'}\")\n",
    "    \n",
    "    if data['trial_balance']:\n",
    "        dates = list(data['trial_balance'].keys())\n",
    "        logger.info(f\"Date Range: {min(dates)} to {max(dates)}\")\n",
    "    \n",
    "    print(\"\\nâœ… Data loading complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"âŒ FATAL ERROR during data loading: {e}\")\n",
    "    logger.error(f\"   Data Path Attempted: {data_path}\")\n",
    "    logger.error(f\"   TB Folder: {tb_folder}\")\n",
    "    logger.error(f\"   TB Folder Exists: {tb_folder.exists()}\")\n",
    "    raise\n",
    "\n",
    "# ========== EXTRACT TO EXPECTED VARIABLE NAMES ==========\n",
    "# The rest of the notebook expects these variable names\n",
    "trial_balance_data = data['trial_balance']\n",
    "chart_of_accounts = data['chart_of_accounts']\n",
    "\n",
    "logger.info(\"\\nâœ… Variables ready:\")\n",
    "logger.info(f\"   - trial_balance_data: {len(trial_balance_data)} dates\")\n",
    "logger.info(f\"   - chart_of_accounts: {len(chart_of_accounts) if chart_of_accounts is not None else 0} records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe999c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919b466",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 4A. Interactive Folder Selection UI\n",
    "\n",
    "Select the year and month folder to load trial balance data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68d7b5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEPRECATED: Interactive Folder Selector UI\n",
    "# This cell is disabled when running via papermill/GUI\n",
    "# The GUI now writes a config file that Cell 13 reads instead\n",
    "\n",
    "print(\"âš ï¸  This interactive folder selector is DISABLED when running via GUI/papermill\")\n",
    "print(\"   The system now uses config file approach (config/run_config.json)\")\n",
    "print(\"   See Cell 13 for the automatic config-based data loading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be41551d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# âš ï¸  DEPRECATED FUNCTION - DO NOT USE\n",
    "# ============================================================\n",
    "# Function: load_trial_balance_data_from_selection()\n",
    "# \n",
    "# REASON FOR DEPRECATION:\n",
    "# - Relied on 'selected_folder' dict from interactive UI widgets\n",
    "# - Interactive widgets don't work with papermill execution\n",
    "# \n",
    "# REPLACED BY:\n",
    "# - Cell 13: Reads config/run_config.json (written by GUI)\n",
    "# - Cell 13: Has automatic fallback to latest year/month\n",
    "# - Cell 13: Uses absolute paths (works in any execution context)\n",
    "#\n",
    "# This function definition is kept for reference but should not be called.\n",
    "# ============================================================\n",
    "\n",
    "logger.info(\"âš ï¸  Cell 17 (load_trial_balance_data_from_selection function) is DEPRECATED\")\n",
    "logger.info(\"   Function not defined - use Cell 13 data loading instead\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91be5b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# âš ï¸  DEPRECATED CELL - DO NOT USE\n",
    "# ============================================================\n",
    "# This cell used the old 'selected_folder' approach with interactive UI\n",
    "# \n",
    "# REASON FOR DEPRECATION:\n",
    "# - Interactive widgets don't work with papermill\n",
    "# - GUI now handles folder selection and writes config file\n",
    "# \n",
    "# NEW APPROACH:\n",
    "# - Cell 13 reads config/run_config.json (written by GUI)\n",
    "# - Cell 13 has automatic fallback to latest year/month if no config\n",
    "# - All paths are absolute (works with papermill execution context)\n",
    "#\n",
    "# IF YOU NEED TO LOAD DATA:\n",
    "# - Use Cell 13 (reads config file and loads data automatically)\n",
    "# - Or run via GUI: scripts/launchers/launch_gui.bat\n",
    "# ============================================================\n",
    "\n",
    "logger.info(\"âš ï¸  Cell 18 (selected_folder loader) is DEPRECATED - skipping\")\n",
    "logger.info(\"   Data loading happens in Cell 13 using config file approach\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3474e483",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load reference data\n",
    "reference_data = load_reference_data()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ“‹ REFERENCE DATA LOADING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Load Time: {reference_data['metadata']['load_timestamp']}\")\n",
    "print(f\"\\nCOA Mapping: {'Loaded' if reference_data['coa_mapping'] is not None else 'Not Loaded'}\")\n",
    "if reference_data['metadata']['coa_mapping_file']:\n",
    "    print(f\"  File: {reference_data['metadata']['coa_mapping_file']}\")\n",
    "print(f\"\\nPortfolio Mapping: {'Loaded' if reference_data['portfolio_mapping'] is not None else 'Not Loaded'}\")\n",
    "if reference_data['metadata']['portfolio_mapping_file']:\n",
    "    print(f\"  File: {reference_data['metadata']['portfolio_mapping_file']}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Extract to expected variable names (for backward compatibility)\n",
    "coa_mapping = reference_data['coa_mapping']\n",
    "portfolio_mapping = reference_data['portfolio_mapping']\n",
    "\n",
    "logger.info(f\"âœ… Reference variables ready:\")\n",
    "logger.info(f\"   - coa_mapping: {len(coa_mapping) if coa_mapping is not None else 0} records\")\n",
    "logger.info(f\"   - portfolio_mapping: {len(portfolio_mapping) if portfolio_mapping is not None else 0} records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a20b8f8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reference_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45519ae",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 5. Separate Data by Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bde5a36",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEPRECATED: Manual folder selection trigger\n",
    "# This cell is disabled when running via papermill/GUI\n",
    "\n",
    "print(\"âš ï¸  Manual folder selection is DISABLED\")\n",
    "print(\"   Data loading now happens automatically in Cell 13\")\n",
    "print(\"   using the config file from the GUI (config/run_config.json)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d0eb87",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 6. Add Date Column to Trial Balance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f52d45b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add 'Date' column to each Trial Balance DataFrame\n",
    "for date_key, df in trial_balance_data.items():\n",
    "    df['Date'] = date_key\n",
    "\n",
    "print(\"âœ“ Date column added to all Trial Balance DataFrames\")\n",
    "print(f\"\\nProcessed {len(trial_balance_data)} date(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc04de6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 7. Consolidate Trial Balance Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60174e8d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Consolidate all Trial Balance DataFrames into a single DataFrame\n",
    "trial_balance_consolidated = pd.concat(trial_balance_data.values(), ignore_index=True)\n",
    "\n",
    "print(\"âœ“ Trial Balance data consolidated\")\n",
    "print(f\"\\nTotal records: {len(trial_balance_consolidated):,}\")\n",
    "print(f\"Date range: {trial_balance_consolidated['Date'].min()} to {trial_balance_consolidated['Date'].max()}\")\n",
    "print(f\"Unique dates: {trial_balance_consolidated['Date'].nunique()}\")\n",
    "print(f\"\\nColumns: {trial_balance_consolidated.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48911200",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(trial_balance_consolidated['Date'].unique())\n",
    "\n",
    "trial_balance_consolidated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56bfefc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### `GET MONTH AND LATEST DAY VALUE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c8814",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_latest_day_per_month(df: pd.DataFrame, date_col: str = 'Date', flat_output: bool = False) -> Union[Dict[str, Dict[str, Any]], Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Processes a DataFrame to find the latest date for each unique reporting month.\n",
    "\n",
    "    If flat_output is False (default), it returns a structured, nested dictionary \n",
    "    with a unique key ('YYYY-Month Name').\n",
    "    \n",
    "    If flat_output is True, it returns a flat dictionary mapping 'Month Name' to \n",
    "    the 'Latest Date'. WARNING: This flat output is NOT robust for multi-year data\n",
    "    as it will overwrite older months with newer ones.\n",
    "\n",
    "    Args:\n",
    "        df: The input DataFrame (e.g., trial_balance_consolidated).\n",
    "        date_col: The name of the date column.\n",
    "        flat_output: If True, returns a simple {'Month Name': 'YYYY-MM-DD'} map, \n",
    "                     losing year context. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        The requested dictionary format.\n",
    "    \"\"\"\n",
    "    if df.empty or date_col not in df.columns:\n",
    "        print(f\"Error: DataFrame is empty or does not contain a '{date_col}' column.\")\n",
    "        return {}\n",
    "\n",
    "    # 1. Ensure the date column is in datetime format and drop invalid rows\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    df = df.dropna(subset=[date_col])\n",
    "\n",
    "    # 2. Group by Year and Month (robust grouping) and find the maximum date in each group.\n",
    "    df_latest_dates = df.groupby([df[date_col].dt.year, df[date_col].dt.month])[date_col].max()\n",
    "\n",
    "    # --- 3. Format the result based on flat_output parameter ---\n",
    "    \n",
    "    if flat_output:\n",
    "        # User requested the simple, flat format (Interpretation A)\n",
    "        latest_dates_flat: Dict[str, str] = {}\n",
    "        for (year, month), latest_date in df_latest_dates.items():\n",
    "            # Uses the month name as the key, which is ambiguous across years\n",
    "            month_name = latest_date.strftime('%B')\n",
    "            latest_dates_flat[month_name] = latest_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # WARNING: The loop inherently takes the LATEST date found for that month name across ALL years.\n",
    "        return latest_dates_flat\n",
    "        \n",
    "    else:\n",
    "        # Default: Structured, unique-key output (Robust)\n",
    "        latest_dates_structured: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "        for (year, month), latest_date in df_latest_dates.items():\n",
    "            # Create a unique primary key: 'YYYY-Month Name'\n",
    "            primary_key = latest_date.strftime('%Y-%B')\n",
    "            \n",
    "            # Create the structured record containing all required keys\n",
    "            latest_dates_structured[primary_key] = {\n",
    "                'date': latest_date.strftime('%Y-%m-%d'),\n",
    "                'month_name': latest_date.strftime('%B'),\n",
    "                'month_num': int(month),\n",
    "                'year': int(year)\n",
    "            }\n",
    "\n",
    "        return latest_dates_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeec616",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Robust, Structured Output (Default)\n",
    "latest_reporting_dates_structured = get_latest_day_per_month(trial_balance_consolidated, flat_output=False) # JSON \n",
    "\n",
    "# 2. Flat Output (As requested, but with data integrity warning)\n",
    "latest_reporting_dates_flat = get_latest_day_per_month(trial_balance_consolidated, flat_output=True) # DICT\n",
    "\n",
    "# latest_reporting_dates_flat\n",
    "latest_reporting_dates_structured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9524ba0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 8. Create Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff4ee7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create pivot table\n",
    "trial_balance_pivot_table = trial_balance_consolidated.pivot_table(\n",
    "    index='accountname',           # Rows: GL Account\n",
    "    columns='level1accountname',   # Columns: Fund Name\n",
    "    values='netamt',               # Values: Balance\n",
    "    aggfunc='sum',                 # Sum the netamt\n",
    "    fill_value=0                   # Fill missing values with 0\n",
    ")\n",
    "\n",
    "# Rename index and columns for clarity\n",
    "trial_balance_pivot_table.index.name = 'GL Account'\n",
    "trial_balance_pivot_table.columns.name = 'Fund Name'\n",
    "\n",
    "print(\"âœ“ Pivot table created\")\n",
    "print(f\"\\nShape: {trial_balance_pivot_table.shape[0]} GL Accounts Ã— {trial_balance_pivot_table.shape[1]} Funds\")\n",
    "print(f\"Total Balance: {trial_balance_pivot_table.sum().sum():,.2f}\")\n",
    "\n",
    "# Display pivot table\n",
    "trial_balance_pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f16bdbd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(trial_balance_pivot_table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2bde79",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def capitalize_pivot_columns(pivot_df):\n",
    "    \"\"\"\n",
    "    Capitalize all column names in the pivot table except for the index ('GL Account').\n",
    "    \n",
    "    Parameters:\n",
    "        pivot_df (DataFrame): The pivot table with Fund Names as columns\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Pivot table with capitalized column names\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_copy = pivot_df.copy()\n",
    "    \n",
    "    # Capitalize all column names\n",
    "    df_copy.columns = [col.upper() for col in df_copy.columns]\n",
    "    \n",
    "    print(\"âœ“ Pivot table columns capitalized\")\n",
    "    print(f\"  Columns: {list(df_copy.columns)}\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Apply the function to the pivot table\n",
    "trial_balance_pivot_table = capitalize_pivot_columns(trial_balance_pivot_table)\n",
    "\n",
    "# Display updated columns\n",
    "print(f\"\\nðŸ“‹ Updated columns: {list(trial_balance_pivot_table.columns)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f2eec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# `Chart of Accounts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295de335",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chart_of_accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6658308",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_unique_column_df(\n",
    "    df: pd.DataFrame, \n",
    "    col_name: str = 'accountname',\n",
    "    sort_ascending: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a new DataFrame containing only the specified column, with all \n",
    "    duplicate values removed, and sorts the result.\n",
    "\n",
    "    Args:\n",
    "        df: The input pandas DataFrame.\n",
    "        col_name: The name of the column to isolate, deduplicate, and sort. \n",
    "                  Defaults to 'accountname'.\n",
    "        sort_ascending: If True, sorts the unique values in ascending order \n",
    "                        (A-Z or 0-9). If False, sorts descending. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        A new DataFrame with a single column (col_name) containing only unique values, sorted.\n",
    "    \"\"\"\n",
    "    if col_name not in df.columns:\n",
    "        print(f\"Error: Column '{col_name}' not found in the DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # 1. Select the single column and create a copy\n",
    "    single_col_df = df[[col_name]].copy()\n",
    "\n",
    "    # 2. Drop duplicate rows based on that column\n",
    "    unique_df = single_col_df.drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    # 3. Sort the unique values (NEW STEP)\n",
    "    # The sort_values method is applied directly to the resulting unique DataFrame.\n",
    "    unique_df = unique_df.sort_values(by=col_name, ascending=sort_ascending).reset_index(drop=True)\n",
    "\n",
    "    return unique_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32503b5a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Your current request: Unique 'accountname' values in a new DataFrame\n",
    "unique_account_df = create_unique_column_df(chart_of_accounts, col_name='accountname')\n",
    "unique_account_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e60fabb",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "coa_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f193d014",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_coa_data(\n",
    "    unique_df: pd.DataFrame, \n",
    "    coa_mapping_df: pd.DataFrame,\n",
    "    left_on_col: str = 'accountname',\n",
    "    right_on_col: str = 'GL Account',\n",
    "    mapping_cols: List[str] = ['TB Account Name', 'Account Type', 'FS Classification']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs a left merge to attach CoA classification data to the unique account list.\n",
    "\n",
    "    Args:\n",
    "        unique_df: The target DataFrame (e.g., from create_unique_column_df) \n",
    "                   containing the unique keys.\n",
    "        coa_mapping_df: The source DataFrame containing the mapping data.\n",
    "        left_on_col: The key column in the unique_df (e.g., 'accountname').\n",
    "        right_on_col: The key column in the coa_mapping_df (e.g., 'GL Account').\n",
    "        mapping_cols: The columns to extract from coa_mapping_df and add to unique_df.\n",
    "\n",
    "    Returns:\n",
    "        The unique_df DataFrame augmented with the mapped columns.\n",
    "    \"\"\"\n",
    "    # 1. Select only the necessary mapping columns plus the join key from the mapping table\n",
    "    cols_to_select = [right_on_col] + mapping_cols\n",
    "    mapping_subset = coa_mapping_df[cols_to_select]\n",
    "    \n",
    "    # 2. Perform a left merge\n",
    "    # A left merge ensures all rows in unique_df (the unique accounts) are kept.\n",
    "    merged_df = pd.merge(\n",
    "        unique_df,\n",
    "        mapping_subset,\n",
    "        left_on=left_on_col,\n",
    "        right_on=right_on_col,\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 3. Clean up the extra join key column that was merged (if keys are different)\n",
    "    # The right_on_col is redundant since left_on_col already contains the key.\n",
    "    if left_on_col != right_on_col and right_on_col in merged_df.columns:\n",
    "        merged_df = merged_df.drop(columns=[right_on_col])\n",
    "        \n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa8be9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "coa_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c597ba",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Merge the Classification Data\n",
    "chart_of_accounts_final = merge_coa_data(unique_account_df, coa_mapping)\n",
    "\n",
    "chart_of_accounts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b6c094",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chart_of_accounts_final.to_excel('../data/processed/chart_of_accounts_11-12-2025.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73684e2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Export Update COA MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc52a29",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_coa_mapping_and_save(\n",
    "    chart_of_accounts_final: pd.DataFrame, \n",
    "    coa_mapping_df: pd.DataFrame,\n",
    "    account_col: str = 'accountname',\n",
    "    gl_account_col: str = 'GL Account',\n",
    "    mapping_cols: List[str] = ['TB Account Name', 'Account Type', 'FS Classification'],\n",
    "    save_path: str = '../data/references/COA Mapping folder',\n",
    "    filename_base: str = 'Chart of Accounts Mapping'\n",
    ") -> Tuple[pd.DataFrame, str]:\n",
    "    \"\"\"\n",
    "    Identifies unclassified accounts, appends them to the CoA mapping table \n",
    "    with a placeholder value, and saves the updated mapping to a file.\n",
    "    \n",
    "    FIX: Now checks the save_path for the latest dated file and loads it \n",
    "    as the base for updates, ensuring manual classifications are preserved.\n",
    "\n",
    "    Args:\n",
    "        chart_of_accounts_final: The result of the merge_coa_data function \n",
    "                                 (unique accounts + classifications, potentially with NaNs).\n",
    "        coa_mapping_df: The original CoA mapping DataFrame (used as a fallback if no saved file is found).\n",
    "        account_col: The account name column in chart_of_accounts_final (default: 'accountname').\n",
    "        gl_account_col: The key column in coa_mapping_df (default: 'GL Account').\n",
    "        mapping_cols: The classification columns that might contain NaNs.\n",
    "        save_path: The directory path to save the updated mapping file.\n",
    "        filename_base: The base name for the output file (e.g., 'Chart of Accounts Mapping').\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing: \n",
    "        1. The updated CoA mapping DataFrame.\n",
    "        2. The full filepath of the saved document.\n",
    "    \"\"\"\n",
    "    # 0. Check for and load the latest saved version first (to include manual updates)\n",
    "    loaded_coa_mapping_df = coa_mapping_df.copy()\n",
    "    \n",
    "    # Create the directory if it doesn't exist (important for robust execution)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Pattern to search for in filenames\n",
    "    base_pattern = f\"{filename_base} as of \"\n",
    "\n",
    "    # Helper function to extract date from filename\n",
    "    def extract_date_from_filename(filename, base):\n",
    "        try:\n",
    "            # Example: \"Chart of Accounts Mapping as of 11.14.2025.xlsx\"\n",
    "            date_str = filename.replace(base, \"\").replace(\".xlsx\", \"\")\n",
    "            return datetime.strptime(date_str, '%m.%d.%Y')\n",
    "        except ValueError:\n",
    "            return datetime.min # Return minimum date for files that don't match the expected format\n",
    "\n",
    "    files = os.listdir(save_path)\n",
    "    candidate_files = [f for f in files if f.startswith(base_pattern) and f.endswith(\".xlsx\")]\n",
    "\n",
    "    if candidate_files:\n",
    "        latest_date = datetime.min\n",
    "        latest_file = None\n",
    "        \n",
    "        for filename in candidate_files:\n",
    "            current_date = extract_date_from_filename(filename, base_pattern)\n",
    "            if current_date > latest_date:\n",
    "                latest_date = current_date\n",
    "                latest_file = filename\n",
    "                \n",
    "        if latest_file:\n",
    "            latest_filepath = os.path.join(save_path, latest_file)\n",
    "            try:\n",
    "                # We assume the external file is the most up-to-date source of truth\n",
    "                loaded_coa_mapping_df = pd.read_excel(latest_filepath)\n",
    "                print(f\"âœ… Replaced input mapping with latest saved file: {latest_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading latest file {latest_filepath}. Proceeding with input coa_mapping_df. Error: {e}\")\n",
    "\n",
    "\n",
    "    # 1. Identify new records (where the first mapping column is NaN)\n",
    "    # This assumes if the first mapping column is NaN, the rest are too (due to the merge).\n",
    "    unclassified_accounts = chart_of_accounts_final[\n",
    "        chart_of_accounts_final[mapping_cols[0]].isna()\n",
    "    ].copy()\n",
    "\n",
    "    if unclassified_accounts.empty:\n",
    "        print(\"No new unclassified accounts found. Mapping table is up-to-date.\")\n",
    "        # Return the loaded file if it was loaded, or the original if not.\n",
    "        return loaded_coa_mapping_df, \"\" \n",
    "\n",
    "    print(f\"Found {len(unclassified_accounts)} new accounts to add to the mapping table.\")\n",
    "\n",
    "    # 2. Prepare new records for concatenation\n",
    "    new_records = unclassified_accounts[[account_col]].copy()\n",
    "    \n",
    "    # Rename the account column to match the mapping table key ('GL Account')\n",
    "    new_records.rename(columns={account_col: gl_account_col}, inplace=True)\n",
    "    \n",
    "    # Fill classification columns with placeholder value for immediate review\n",
    "    # The original placeholder ' ' is used here, as per your previous version\n",
    "    placeholder_value = ' ' \n",
    "    for col in mapping_cols:\n",
    "        new_records[col] = placeholder_value\n",
    "    \n",
    "    # 3. Append new records to the loaded (or original) mapping table\n",
    "    updated_coa_mapping_df = pd.concat([loaded_coa_mapping_df, new_records], ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates from the GL Account key (in case an old unclassified was manually added, but reappears)\n",
    "    updated_coa_mapping_df = updated_coa_mapping_df.drop_duplicates(subset=[gl_account_col], keep='last')\n",
    "    \n",
    "    # Optional: Sort the new mapping file by GL Account for better readability\n",
    "    updated_coa_mapping_df = updated_coa_mapping_df.sort_values(by=gl_account_col).reset_index(drop=True)\n",
    "\n",
    "    # 4. Save the updated mapping file\n",
    "    date_exported = datetime.now().strftime('%m.%d.%Y')\n",
    "    filename = f\"{filename_base} as of {date_exported}.xlsx\" \n",
    "    \n",
    "    # Use os.path.join for cross-platform path construction\n",
    "    full_filepath = os.path.join(save_path, filename)\n",
    "    \n",
    "    try:\n",
    "        updated_coa_mapping_df.to_excel(full_filepath, index=False)\n",
    "        print(f\"Successfully saved updated mapping to: {full_filepath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving file to {full_filepath}: {e}\")\n",
    "        full_filepath = f\"Error saving file: {e}\" # Indicate error in return path\n",
    "\n",
    "    return updated_coa_mapping_df, full_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005bd20",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# C. Load latest saved map OR use initial map, then update and save.\n",
    "# We explicitly tell the function that the input key is 'accountname', but the output map must be 'GL Account'\n",
    "master_mapping_df, saved_path = update_coa_mapping_and_save( # Changed function name here\n",
    "    chart_of_accounts_final, \n",
    "    coa_mapping, \n",
    "    account_col='accountname', \n",
    "    gl_account_col='GL Account',\n",
    "    save_path='../data/references/COA Mapping', # Using a local folder for the example\n",
    "    filename_base='Chart of Accounts Mapping'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19290b8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "coa_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d861695",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "chart_of_accounts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809db2c0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Update the mapping and save the new file\n",
    "# updated_coa_mapping, saved_path = update_coa_mapping_and_save(\n",
    "#     chart_of_accounts_final, \n",
    "#     coa_mapping, \n",
    "#     save_path='../data/references/COA Mapping', # Using a local folder for the example\n",
    "#     filename_base='Chart of Accounts Mapping'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041607d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load reference data\n",
    "reference_data = load_reference_data()\n",
    "\n",
    "# Reference data\n",
    "coa_mapping = reference_data['coa_mapping']\n",
    "\n",
    "coa_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4429f7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 9. Match GL Accounts with COA Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493b0cf5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Get GL Accounts from pivot table (index)\n",
    "# pivot_gl_accounts = set(trial_balance_pivot_table.index)\n",
    "\n",
    "# # Get GL Accounts from COA Mapping\n",
    "# coa_gl_accounts = set(coa_mapping['GL Account'])\n",
    "\n",
    "# # Find accounts in pivot table that are NOT in COA Mapping\n",
    "# missing_in_coa = pivot_gl_accounts - coa_gl_accounts\n",
    "\n",
    "# # Find accounts in COA Mapping that are NOT in pivot table\n",
    "# missing_in_pivot = coa_gl_accounts - pivot_gl_accounts\n",
    "\n",
    "# print(\"=\"*60)\n",
    "# print(\"GL ACCOUNT MATCHING ANALYSIS\")\n",
    "# print(\"=\"*60)\n",
    "# print(f\"\\nðŸ“Š Total GL Accounts in Pivot Table: {len(pivot_gl_accounts)}\")\n",
    "# print(f\"ðŸ“Š Total GL Accounts in COA Mapping: {len(coa_gl_accounts)}\")\n",
    "# print(f\"\\nâœ“ Matching Accounts: {len(pivot_gl_accounts & coa_gl_accounts)}\")\n",
    "# print(f\"âš ï¸  Accounts in Pivot but NOT in COA Mapping: {len(missing_in_coa)}\")\n",
    "# print(f\"â„¹ï¸  Accounts in COA Mapping but NOT in Pivot: {len(missing_in_pivot)}\")\n",
    "\n",
    "# # Display missing accounts\n",
    "# if missing_in_coa:\n",
    "#     print(\"\\n\" + \"=\"*60)\n",
    "#     print(\"âš ï¸  NEW ACCOUNTS FOUND (Need to be added to COA Mapping):\")\n",
    "#     print(\"=\"*60)\n",
    "#     for i, account in enumerate(sorted(missing_in_coa), 1):\n",
    "#         print(f\"{i:3}. {account}\")\n",
    "# else:\n",
    "#     print(\"\\nâœ“ All accounts in pivot table exist in COA Mapping!\")\n",
    "\n",
    "# # Create indicator DataFrame for new accounts\n",
    "# if missing_in_coa:\n",
    "#     new_accounts_df = pd.DataFrame({\n",
    "#         'GL Account': sorted(missing_in_coa),\n",
    "#         'Status': 'NEW - Not in COA Mapping',\n",
    "#         'TB Account Name': '',\n",
    "#         'Account Type': '',\n",
    "#         'FS Classification': ''\n",
    "#     })\n",
    "    \n",
    "#     print(f\"\\nðŸ“ Created DataFrame with {len(new_accounts_df)} new account(s) to be added\")\n",
    "#     print(\"    Variable: new_accounts_df\")\n",
    "# else:\n",
    "#     new_accounts_df = None\n",
    "#     print(\"\\nâœ“ No new accounts to add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f7e43",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Display new accounts DataFrame\n",
    "# if new_accounts_df is not None:\n",
    "#     print(f\"ðŸ“‹ New Accounts to Add to COA Mapping ({len(new_accounts_df)} accounts):\\n\")\n",
    "#     display(new_accounts_df)\n",
    "# else:\n",
    "#     print(\"âœ“ No new accounts found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c625515b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create updated COA Mapping with new accounts inserted\n",
    "# if new_accounts_df is not None:\n",
    "#     # Combine original COA mapping with new accounts\n",
    "#     updated_coa_mapping = pd.concat([coa_mapping, new_accounts_df], ignore_index=True)\n",
    "    \n",
    "#     # Sort by GL Account for better organization\n",
    "#     updated_coa_mapping = updated_coa_mapping.sort_values('GL Account').reset_index(drop=True)\n",
    "    \n",
    "#     print(\"âœ“ Updated COA Mapping created with new accounts\")\n",
    "#     print(f\"\\nðŸ“Š Original COA Mapping: {len(coa_mapping)} accounts\")\n",
    "#     print(f\"ðŸ“Š New Accounts Added: {len(new_accounts_df)} accounts\")\n",
    "#     print(f\"ðŸ“Š Updated COA Mapping: {len(updated_coa_mapping)} accounts\")\n",
    "#     print(f\"\\nðŸ’¾ Variable: updated_coa_mapping\")\n",
    "    \n",
    "#     # Create indicator column to show which accounts are new\n",
    "#     updated_coa_mapping['Is_New_Account'] = updated_coa_mapping['GL Account'].isin(missing_in_coa)\n",
    "    \n",
    "#     print(f\"\\nâœ“ Added 'Is_New_Account' indicator column\")\n",
    "#     print(f\"   - True: Account is newly found (not in original COA Mapping)\")\n",
    "#     print(f\"   - False: Account existed in original COA Mapping\")\n",
    "# else:\n",
    "#     updated_coa_mapping = coa_mapping.copy()\n",
    "#     updated_coa_mapping['Is_New_Account'] = False\n",
    "#     print(\"âœ“ No new accounts to add - using original COA Mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c537fb71",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Display updated COA Mapping - showing only new accounts\n",
    "# print(\"ðŸ“‹ Updated COA Mapping - New Accounts Only:\\n\")\n",
    "# display(updated_coa_mapping[updated_coa_mapping['Is_New_Account'] == True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d7786",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Export updated COA Mapping if new accounts were added\n",
    "# if new_accounts_df is not None and len(new_accounts_df) > 0:\n",
    "#     # Define export path\n",
    "#     export_folder = Path('../data/references/COA Mapping')\n",
    "#     export_folder.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "#     # Create filename with current date (MM.DD.YYYY format)\n",
    "#     current_date = datetime.now().strftime('%m.%d.%Y')\n",
    "#     export_filename = f'Chart of Accounts Mapping as of {current_date}.xlsx'\n",
    "#     export_path = export_folder / export_filename\n",
    "    \n",
    "#     # Export to Excel\n",
    "#     updated_coa_mapping.to_excel(export_path, index=False, engine='openpyxl')\n",
    "    \n",
    "#     print(\"=\"*60)\n",
    "#     print(\"ðŸ“¤ EXPORT SUCCESSFUL\")\n",
    "#     print(\"=\"*60)\n",
    "#     print(f\"âœ“ File exported to: {export_path}\")\n",
    "#     print(f\"âœ“ Filename: {export_filename}\")\n",
    "#     print(f\"âœ“ Total records: {len(updated_coa_mapping)}\")\n",
    "#     print(f\"âœ“ New accounts added: {len(new_accounts_df)}\")\n",
    "#     print(f\"âœ“ Export timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "#     print(\"\\nðŸ’¡ Note: The 'Is_New_Account' column indicates which accounts are newly added (True)\")\n",
    "# else:\n",
    "#     print(\"â„¹ï¸  No new accounts to export - COA Mapping unchanged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151c1fd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chart_of_accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aacf119",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# coa_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39bb838",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# chart_of_accounts_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa8b161",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def capitalize_portfolio_mapping(portfolio_df):\n",
    "    \"\"\"\n",
    "    Capitalize all values in the 'level1accountname' column of the portfolio mapping.\n",
    "    \n",
    "    Parameters:\n",
    "        portfolio_df (DataFrame): The portfolio mapping DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Portfolio mapping with capitalized 'level1accountname' values, or None if input is None\n",
    "    \"\"\"\n",
    "    # Handle None case\n",
    "    if portfolio_df is None:\n",
    "        print(\"âš ï¸  WARNING: portfolio_mapping is None - no data to capitalize\")\n",
    "        return None\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df_copy = portfolio_df.copy()\n",
    "    \n",
    "    # Find the correct column name (it has a tab character)\n",
    "    level1_col = [col for col in df_copy.columns if 'level1accountname' in col.lower()]\n",
    "    \n",
    "    if level1_col:\n",
    "        col_name = level1_col[0]\n",
    "        df_copy[col_name] = df_copy[col_name].str.upper()\n",
    "        print(f\"âœ“ Portfolio mapping '{col_name}' column capitalized\")\n",
    "        print(f\"  Updated {len(df_copy)} row(s)\")\n",
    "        print(f\"  Unique values: {df_copy[col_name].nunique()}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  WARNING: 'level1accountname' column not found in portfolio_mapping\")\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Apply the function to portfolio_mapping\n",
    "portfolio_mapping = capitalize_portfolio_mapping(portfolio_mapping)\n",
    "\n",
    "# Display updated portfolio mapping\n",
    "if portfolio_mapping is not None and len([col for col in portfolio_mapping.columns if 'level1accountname' in col.lower()]) > 0:\n",
    "    level1_col = [col for col in portfolio_mapping.columns if 'level1accountname' in col.lower()][0]\n",
    "    print(f\"\\nðŸ“‹ Updated portfolio_mapping unique values:\")\n",
    "    print(portfolio_mapping[level1_col].unique().tolist())\n",
    "else:\n",
    "    print(\"\\nâ„¹ï¸  No portfolio mapping data available to display\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6075f07c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "portfolio_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f34519b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial_balance_pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d687e101",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rename_pivot_columns_with_fund_code_v2(pivot_df, portfolio_df):\n",
    "    \"\"\"\n",
    "    Rename pivot table columns based on portfolio mapping.\n",
    "    Maps level1accountname to Fund_Code from portfolio_mapping using \n",
    "    case-insensitive and whitespace-agnostic matching (UPPERCASE + Strip).\n",
    "    \n",
    "    Parameters:\n",
    "        pivot_df (DataFrame): The pivot table with fund names as columns\n",
    "        portfolio_df (DataFrame): The portfolio mapping DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Pivot table with renamed columns (Fund Codes)\n",
    "    \"\"\"\n",
    "    if portfolio_df is None:\n",
    "        print(\"âš ï¸  WARNING: portfolio_mapping is None - cannot rename columns\")\n",
    "        return pivot_df\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    df_copy = pivot_df.copy()\n",
    "    \n",
    "    # --- 1. Find the necessary columns (same as original) ---\n",
    "    level1_col = [col for col in portfolio_df.columns if 'level1accountname' in col.lower()]\n",
    "    fund_code_col = [col for col in portfolio_df.columns if 'fund_code' in col.lower() or 'fund code' in col.lower()]\n",
    "    \n",
    "    if not level1_col or not fund_code_col:\n",
    "        print(f\"âš ï¸  WARNING: Required column not found. Level1 found: {bool(level1_col)}, Fund_Code found: {bool(fund_code_col)}\")\n",
    "        return pivot_df\n",
    "    \n",
    "    level1_col_name = level1_col[0]\n",
    "    fund_code_col_name = fund_code_col[0]\n",
    "    \n",
    "    # --- 2. Create MAPPING DICTIONARY (Key: UPPER/Strip Fund Name, Value: Fund Code) ---\n",
    "    # The key is the standardized name we expect from pivot_df columns\n",
    "    mapping_dict = dict(zip(\n",
    "        portfolio_df[level1_col_name].str.strip().str.upper(),\n",
    "        portfolio_df[fund_code_col_name]\n",
    "    ))\n",
    "    \n",
    "    # --- 3. Rename Columns: Apply Standardization for Lookup ---\n",
    "    \n",
    "    # The core change is here: we standardize the column name (col) before looking it up.\n",
    "    def get_new_col_name(old_col_name):\n",
    "        # Standardize the pivot table column name to match the dictionary key\n",
    "        standardized_name = str(old_col_name).strip().upper()\n",
    "        \n",
    "        # Look up the new name. If not found, return the original column name.\n",
    "        return mapping_dict.get(standardized_name, old_col_name)\n",
    "\n",
    "    new_columns = [get_new_col_name(col) for col in df_copy.columns]\n",
    "    \n",
    "    df_copy.columns = new_columns\n",
    "    \n",
    "    # --- 4. Logging and Return (same as original, adjusted for v2) ---\n",
    "    \n",
    "    # Optional: You can still print the mapping dictionary for verification\n",
    "    print(f\"ðŸ“‹ Column Mapping Dictionary (Keys are UPPER/Strip):\")\n",
    "    for old_name, new_name in list(mapping_dict.items())[:5]: # Print first 5 for brevity\n",
    "        print(f\" Â {old_name} â†’ {new_name}\")\n",
    "    if len(mapping_dict) > 5:\n",
    "        print(\" Â ...\")\n",
    "    \n",
    "    renamed_count = sum(1 for old, new in zip(pivot_df.columns, df_copy.columns) if old != new)\n",
    "\n",
    "    print(f\"\\nâœ“ Pivot table columns renamed\")\n",
    "    print(f\" Â Original columns (first 5): {list(pivot_df.columns)[:5]}\")\n",
    "    print(f\" Â New columns (first 5): {list(df_copy.columns)[:5]}\")\n",
    "    print(f\" Â Renamed {renamed_count} column(s)\")\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5611c2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the function to rename columns\n",
    "trial_balance_pivot_table = rename_pivot_columns_with_fund_code_v2(trial_balance_pivot_table, portfolio_mapping)\n",
    "\n",
    "# Display updated pivot table\n",
    "print(f\"\\nðŸ“Š Updated Pivot Table Columns:\")\n",
    "print(list(trial_balance_pivot_table.columns)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef3a1c5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "portfolio_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6acd9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(trial_balance_pivot_table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fe581",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display the renamed pivot table\n",
    "trial_balance_pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecfff65",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop_zero_value_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filters and removes rows from a DataFrame where the sum of all numerical \n",
    "    columns is zero. This efficiently removes 'zero-value records' without\n",
    "    creating or dropping temporary columns.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame (e.g., a pivot table).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with rows containing only zero numerical\n",
    "                      values removed.\n",
    "    \"\"\"\n",
    "    # 1. Select only numerical columns (robust handling for mixed types)\n",
    "    # This prevents errors if non-numeric columns (like account descriptions) exist.\n",
    "    numeric_df = df.select_dtypes(include=np.number)\n",
    "\n",
    "    if numeric_df.empty:\n",
    "        print(\"âš ï¸  WARNING: No numerical columns found to sum. Returning original DataFrame.\")\n",
    "        return df\n",
    "\n",
    "    # 2. Calculate the row sum (axis=1) of only the numerical columns.\n",
    "    # This Series acts as the 'helper_col' logic but without adding it to the DF.\n",
    "    row_sum = numeric_df.sum(axis=1)\n",
    "    \n",
    "    # 3. Create a boolean mask: keep rows where the absolute sum is NOT zero.\n",
    "    # We use abs() to handle potential tiny floating point errors, though often \n",
    "    # unnecessary for standard pivot tables.\n",
    "    mask = (row_sum.abs() != 0)\n",
    "    \n",
    "    # 4. Apply the mask to the original DataFrame and return the filtered copy.\n",
    "    df_cleaned = df[mask].copy()\n",
    "    \n",
    "    print(f\"\\nðŸ—‘ï¸  Row Filtering Complete\")\n",
    "    print(f\" Â Original Rows: {len(df)}\")\n",
    "    print(f\" Â Rows Retained: {len(df_cleaned)}\")\n",
    "    print(f\" Â Rows Dropped (Zero Value): {len(df) - len(df_cleaned)}\")\n",
    "    \n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c106319",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the function to drop zero-value rows\n",
    "trial_balance_pivot_table = drop_zero_value_rows(trial_balance_pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c2edc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial_balance_pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e57f6a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_coa_details(df: pd.DataFrame, coa_df: pd.DataFrame, merge_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges COA details ('TB Account Name', 'Account Type') into the pivot table\n",
    "    based on 'GL Account' and places the new columns as the initial columns.\n",
    "    \n",
    "    The merge is performed using a standardized (stripped, upper) temporary key \n",
    "    to ensure case- and whitespace-agnostic matching.\n",
    "    \n",
    "    FIX: Now includes reset_index() for the pivot table, as the GL Account \n",
    "    is typically in the Index after pivoting.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The target pivot table (e.g., trial_balance_pivot_table).\n",
    "        coa_df (pd.DataFrame): The source COA mapping DataFrame.\n",
    "        merge_cols (List[str]): List of columns to insert from coa_df (e.g., ['TB Account Name', 'Account Type']).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with COA details merged and reordered.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ”— Starting COA Details Merge...\")\n",
    "    \n",
    "    # --- PIVOT TABLE FIX: Move Index (GL Account) to a Column ---\n",
    "    # We assume GL Account is in the index due to the KeyError and the column names\n",
    "    if df.index.name is not None:\n",
    "        df = df.reset_index()\n",
    "    elif df.index.names != [None] * len(df.index.names):\n",
    "        # Handle MultiIndex case\n",
    "        df = df.reset_index()\n",
    "    else:\n",
    "        # If the index is unnamed, we must guess it's the first column in the coa_df\n",
    "        # We rely on the name standardization below to correctly pick up the column name\n",
    "        pass\n",
    "\n",
    "    # --- ULTIMATE FIX: Standardize all column names first (Case + Strip) ---\n",
    "    # Convert all column names to stripped UPPERCASE. This makes column access robust.\n",
    "    df.columns = [str(col).strip().upper() for col in df.columns]\n",
    "    coa_df.columns = [str(col).strip().upper() for col in coa_df.columns]\n",
    "    \n",
    "    # --- CONFIRMATION OF STANDARDIZATION (Internal Check) ---\n",
    "    print(f\" Â DF Columns after standardization: {df.columns.tolist()[:3]}...\")\n",
    "    print(f\" Â COA Columns after standardization: {coa_df.columns.tolist()[:3]}...\")\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    # Define the standardized column name for GL Account\n",
    "    GL_ACCOUNT_COL = 'GL ACCOUNT'\n",
    "    \n",
    "    # Standardize the merge_cols so they can be looked up in the coa_df\n",
    "\n",
    "    # --- 1. Robust Key Creation ---\n",
    "    coa_df = coa_df.copy() # Work on copy of coa_df to add temp column\n",
    "    \n",
    "    # Now that the column names are standardized AND index is reset, we access them using GL_ACCOUNT_COL\n",
    "    # We still strip/upper the VALUE within the column for the robust merge KEY\n",
    "    df['__merge_key'] = df[GL_ACCOUNT_COL].astype(str).str.strip().str.upper()\n",
    "    coa_df['__merge_key'] = coa_df[GL_ACCOUNT_COL].astype(str).str.strip().str.upper()\n",
    "    \n",
    "    # --- 2. Prepare COA subset for merge ---\n",
    "    \n",
    "    # Create a list of columns to retrieve from COA, using their uppercase names\n",
    "    # to access the DataFrame, but using the original name (or close match) \n",
    "    # to maintain the desired output header names.\n",
    "    \n",
    "    coa_upper_cols = [str(col).strip().upper() for col in merge_cols]\n",
    "    \n",
    "    # The actual columns we will merge: the merge key + the COA columns\n",
    "    coa_final_cols = ['__merge_key'] + coa_upper_cols\n",
    "    coa_subset = coa_df[coa_final_cols].drop_duplicates(subset=['__merge_key']).copy()\n",
    "    \n",
    "    # Rename the columns in the subset back to the desired output names for the merge\n",
    "    # We rename the uppercase columns back to the mixed-case names (e.g. 'TB ACCOUNT NAME' -> 'TB Account Name')\n",
    "    coa_rename_dict = dict(zip(coa_upper_cols, merge_cols))\n",
    "    coa_subset = coa_subset.rename(columns=coa_rename_dict)\n",
    "    \n",
    "    # --- 3. Perform Left Merge ---\n",
    "    merged_df = pd.merge(\n",
    "        df, coa_subset,\n",
    "        on='__merge_key',\n",
    "        how='left',\n",
    "        validate='many_to_one'\n",
    "    )\n",
    "    \n",
    "    # Drop the temporary merge key column\n",
    "    merged_df = merged_df.drop(columns=['__merge_key'])\n",
    "    \n",
    "    # --- 4. Column Reordering ---\n",
    "    \n",
    "    # We need to find the standardized GL Account column and put it after the new columns.\n",
    "    \n",
    "    # The list of columns we want to be placed first\n",
    "    initial_cols = merge_cols + [col for col in merged_df.columns if col == GL_ACCOUNT_COL]\n",
    "\n",
    "    # Get the rest of the columns\n",
    "    remaining_cols = [col for col in merged_df.columns if col not in initial_cols and col not in merge_cols]\n",
    "            \n",
    "    # Define the final order: new columns, GL Account, then the rest\n",
    "    final_cols = initial_cols + remaining_cols\n",
    "    \n",
    "    df_final = merged_df[final_cols].copy()\n",
    "    \n",
    "    # # Sort the result by the primary descriptive column (as requested) ---\n",
    "    # sort_column = merge_cols[0]\n",
    "    # df_final = df_final.sort_values(by=sort_column).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"âœ“ Merge complete. Columns inserted: {merge_cols}\")\n",
    "    print(f\" Â New initial columns: {df_final.columns[:len(merge_cols) + 1].tolist()}\")\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd82534",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Apply the merge and reorder function\n",
    "trial_balance_pivot_table = merge_coa_details(\n",
    "    trial_balance_pivot_table, \n",
    "    coa_mapping, \n",
    "    ['TB Account Name', 'Account Type']\n",
    ")\n",
    "\n",
    "trial_balance_pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2230189c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial_balance_pivot_table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06047918",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Creation of TB FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee52a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_deduplicated_mapping_subset(\n",
    "    df: pd.DataFrame, \n",
    "    subset_cols: List[str], \n",
    "    deduplicate_on_col: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a new DataFrame containing a subset of columns, deduplicated based \n",
    "    on a specified key column.\n",
    "\n",
    "    This is useful for extracting unique high-level classifications \n",
    "    (like TB Account Name and Account Type) from a detailed transaction/pivot table \n",
    "    or a COA mapping table.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The source DataFrame (e.g., final merged pivot table).\n",
    "        subset_cols (List[str]): The list of columns to include in the output DF.\n",
    "        deduplicate_on_col (str): The column used to drop duplicates.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the unique combinations of the subset columns, \n",
    "                      sorted by the deduplicate key.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Validate required columns exist\n",
    "    missing_cols = [col for col in subset_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: Missing required columns for subset: {missing_cols}\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # 2. Select the subset and create a copy\n",
    "    subset_df = df[subset_cols].copy()\n",
    "\n",
    "    # 3. Deduplicate based on the primary key, keeping the first occurrence\n",
    "    # If there are conflicts (same deduplicate_on_col with different values in other subset_cols),\n",
    "    # this will keep the first instance encountered.\n",
    "    tb_fs_df = subset_df.drop_duplicates(\n",
    "        subset=[deduplicate_on_col], \n",
    "        keep='first'\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    # 4. Sort the result for clean presentation\n",
    "    tb_fs_df = tb_fs_df.sort_values(by=deduplicate_on_col).reset_index(drop=True)\n",
    "    \n",
    "    return tb_fs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241e5b6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial_balance_consolidated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beba310e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- STAGE 3: CREATE DEDUPLICATED MAPPING SUBSET (Your request) ---\n",
    "tb_fs_df = create_deduplicated_mapping_subset(\n",
    "    df=trial_balance_pivot_table,\n",
    "    subset_cols=['TB Account Name', 'Account Type'],\n",
    "    deduplicate_on_col='TB Account Name'\n",
    ")\n",
    "\n",
    "tb_fs_df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda8b78",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "coa_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327be0f0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_fs_classification(\n",
    "    tb_fs_df: pd.DataFrame, \n",
    "    coa_mapping_df: pd.DataFrame,\n",
    "    tb_acct_name_col: str = 'TB Account Name',\n",
    "    acct_type_col: str = 'Account Type',\n",
    "    fs_classification_col: str = 'FS Classification'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges the Financial Statement (FS) Classification column from the COA mapping\n",
    "    into the deduplicated summary table (tb_fs_df), using a robust two-column join.\n",
    "    \n",
    "    The join is performed by standardizing (stripping and uppercasing) the values \n",
    "    in the key columns across both DataFrames to ensure case-insensitive matching.\n",
    "\n",
    "    Args:\n",
    "        tb_fs_df (pd.DataFrame): The target summary DataFrame (e.g., deduplicated \n",
    "                                 by TB Account Name and Account Type).\n",
    "        coa_mapping_df (pd.DataFrame): The source mapping DataFrame containing \n",
    "                                       the FS Classification.\n",
    "        tb_acct_name_col (str): Name of the 'TB Account Name' column in both DFs.\n",
    "        acct_type_col (str): Name of the 'Account Type' column in both DFs.\n",
    "        fs_classification_col (str): Name of the target classification column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The tb_fs_df with the FS Classification column added.\n",
    "    \"\"\"\n",
    "    print(\"\\nðŸ”— Starting Two-Column Merge for FS Classification...\")\n",
    "\n",
    "    # Define the keys for merging\n",
    "    join_keys = [tb_acct_name_col, acct_type_col]\n",
    "    \n",
    "    # Calculate the UPPERCASE keys required for indexing the coa_mapping_df \n",
    "    # (which has its column names uppercased by merge_coa_details)\n",
    "    upper_join_keys = [k.upper().strip() for k in join_keys]\n",
    "    upper_fs_classification_key = fs_classification_col.upper().strip()\n",
    "    \n",
    "    # --- 1. Prepare COA Mapping Subset (Source) ---\n",
    "    try:\n",
    "        # Select columns from coa_mapping_df using the correct UPPERCASE names\n",
    "        coa_subset_source = coa_mapping_df[upper_join_keys + [upper_fs_classification_key]].copy()\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Could not find required columns in COA Mapping DF. Keys attempted: {upper_join_keys + [upper_fs_classification_key]}.\")\n",
    "        print(f\"Available COA columns: {coa_mapping_df.columns.tolist()}\")\n",
    "        print(f\"Original Error: {e}\")\n",
    "        return tb_fs_df # Return original DF on error\n",
    "    \n",
    "    # Rename the columns back to the mixed-case standard for consistent internal processing \n",
    "    # and compatibility with tb_fs_df\n",
    "    coa_subset = coa_subset_source.rename(columns={\n",
    "        upper_join_keys[0]: tb_acct_name_col,\n",
    "        upper_join_keys[1]: acct_type_col,\n",
    "        upper_fs_classification_key: fs_classification_col\n",
    "    })\n",
    "    \n",
    "    # Create temporary standardized key columns (UPPERCASE VALUE, stripped)\n",
    "    TEMP_KEY_1 = '__KEY_TB_ACCT'\n",
    "    TEMP_KEY_2 = '__KEY_ACCT_TYPE'\n",
    "    \n",
    "    # Deduplicate the COA subset on the combined key, keeping the first classification found\n",
    "    # We must create the standardized keys first\n",
    "    coa_subset[TEMP_KEY_1] = coa_subset[tb_acct_name_col].astype(str).str.strip().str.upper()\n",
    "    coa_subset[TEMP_KEY_2] = coa_subset[acct_type_col].astype(str).str.strip().str.upper()\n",
    "    \n",
    "    coa_merge_source = coa_subset.drop_duplicates(\n",
    "        subset=[TEMP_KEY_1, TEMP_KEY_2], \n",
    "        keep='first'\n",
    "    )[[TEMP_KEY_1, TEMP_KEY_2, fs_classification_col]]\n",
    "    \n",
    "    \n",
    "    # --- 2. Prepare Target DF (tb_fs_df) ---\n",
    "    tb_fs_df_out = tb_fs_df.copy()\n",
    "    \n",
    "    try:\n",
    "        # Create the same temporary standardized keys in the target DF\n",
    "        tb_fs_df_out[TEMP_KEY_1] = tb_fs_df_out[tb_acct_name_col].astype(str).str.strip().str.upper()\n",
    "        tb_fs_df_out[TEMP_KEY_2] = tb_fs_df_out[acct_type_col].astype(str).str.strip().str.upper()\n",
    "    except KeyError:\n",
    "        print(f\"Error: Missing key columns in Target DF. Required: {join_keys}\")\n",
    "        return tb_fs_df # Return original DF on error\n",
    "\n",
    "    # --- 3. Perform Left Merge ---\n",
    "    merged_df = pd.merge(\n",
    "        tb_fs_df_out,\n",
    "        coa_merge_source,\n",
    "        left_on=[TEMP_KEY_1, TEMP_KEY_2],\n",
    "        right_on=[TEMP_KEY_1, TEMP_KEY_2],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # --- 4. Cleanup and Reordering ---\n",
    "    \n",
    "    # Drop the temporary merge key columns\n",
    "    merged_df = merged_df.drop(columns=[TEMP_KEY_1, TEMP_KEY_2])\n",
    "    \n",
    "    # Reorder columns to put the new classification next to the Account Type\n",
    "    current_cols = merged_df.columns.tolist()\n",
    "    \n",
    "    # Find indices of key columns\n",
    "    try:\n",
    "        idx_acct_type = current_cols.index(acct_type_col)\n",
    "        # Move the new column next to Account Type\n",
    "        new_cols = current_cols[:]\n",
    "        if fs_classification_col in new_cols:\n",
    "             new_cols.remove(fs_classification_col)\n",
    "             new_cols.insert(idx_acct_type + 1, fs_classification_col)\n",
    "             merged_df = merged_df[new_cols]\n",
    "    except ValueError:\n",
    "        # If column not found, just keep the default order (FS Classification will be last)\n",
    "        pass \n",
    "        \n",
    "    # --- ADDED: Final Sort by Account Type and then TB Account Name ---\n",
    "    try:\n",
    "        # Sort by Account Type first (to group) and then by TB Account Name (for internal alphabetical order)\n",
    "        merged_df = merged_df.sort_values(by=[acct_type_col, tb_acct_name_col]).reset_index(drop=True)\n",
    "    except KeyError:\n",
    "        print(f\"Warning: Could not sort by '{acct_type_col}' or '{tb_acct_name_col}'. Sorting skipped.\")\n",
    "    \n",
    "    print(f\"âœ“ FS Classification merged successfully. Column added: '{fs_classification_col}'\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b3de12",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_classification_to_detailed_df(\n",
    "    detailed_df: pd.DataFrame, \n",
    "    classified_keys_df: pd.DataFrame, \n",
    "    merge_keys: List[str] = ['TB Account Name', 'Account Type'], \n",
    "    classification_col: str = 'FS Classification'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs a left merge to transfer the Financial Statement (FS) Classification \n",
    "    from a small key list back onto the large, detailed DataFrame.\n",
    "\n",
    "    This is necessary when the detailed data source (like the pivot table) is \n",
    "    missing the FS classification but already contains the necessary TB Account \n",
    "    Name and Account Type columns.\n",
    "\n",
    "    Args:\n",
    "        detailed_df (pd.DataFrame): The target DataFrame (e.g., trial_balance_pivot_table) \n",
    "                                    that needs the FS Classification column added.\n",
    "        classified_keys_df (pd.DataFrame): The small, pre-classified DF from a \n",
    "                                           previous step (e.g., the output of merge_fs_classification).\n",
    "        merge_keys (List[str]): The columns used for joining (defaults to \n",
    "                                ['TB Account Name', 'Account Type']).\n",
    "        classification_col (str): The column to be merged (defaults to 'FS Classification').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The detailed_df with the FS Classification column added.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ”— Starting Merge-Back of '{classification_col}' to Detailed DF...\")\n",
    "    \n",
    "    # 1. Prepare the source: select only the keys and the classification column\n",
    "    source_cols = merge_keys + [classification_col]\n",
    "    \n",
    "    # 2. Drop duplicates in the classified keys DF to ensure a clean many-to-one merge\n",
    "    source_df = classified_keys_df[source_cols].drop_duplicates(subset=merge_keys, keep='first').copy()\n",
    "    \n",
    "    # 3. Perform the merge\n",
    "    merged_df = pd.merge(\n",
    "        detailed_df, \n",
    "        source_df, \n",
    "        on=merge_keys, \n",
    "        how='left',\n",
    "        validate='many_to_one'\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Classification '{classification_col}' successfully merged back into detailed DataFrame.\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06d46a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- STAGE 4: MERGE FS CLASSIFICATION (User's request) ---\n",
    "# The new function is called here\n",
    "tb_fs_df = merge_fs_classification(\n",
    "    tb_fs_df=tb_fs_df,\n",
    "    coa_mapping_df=coa_mapping\n",
    ")\n",
    "\n",
    "tb_fs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece2a9f9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tb_fs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee981ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- NEW STAGE 4.5: MERGE CLASSIFICATION BACK TO DETAILED DF (THE MISSING STEP) ---\n",
    "# We use the new helper function to correct the detailed table.\n",
    "trial_balance_pivot_table = merge_classification_to_detailed_df(\n",
    "    detailed_df=trial_balance_pivot_table,\n",
    "    classified_keys_df=tb_fs_df\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6313f862",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trial_balance_pivot_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4faa5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_summary_totals(\n",
    "    df: pd.DataFrame, \n",
    "    group_cols: List[str] = ['FS Classification', 'Account Type', 'TB Account Name']\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregates the detailed Trial Balance (or pivot table) by the specified \n",
    "    classification columns, summing all numerical columns.\n",
    "\n",
    "    This generates the final, high-level Financial Statement summary table. \n",
    "    It is robust against missing grouping columns by using only the columns \n",
    "    that are actually present in the input DataFrame and prevents non-grouping \n",
    "    key columns (like GL ACCOUNT) from being erroneously summed.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The detailed DataFrame (e.g., final_merged_pivot_table) \n",
    "                           containing both classification keys and numerical values.\n",
    "        group_cols (List[str]): The columns to group by. Defaults to the full \n",
    "                                classification hierarchy for the final report.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary table with one row per unique group, containing \n",
    "                      the sum of all numerical values.\n",
    "    \"\"\"\n",
    "    print(f\"\\nðŸ“ˆ Starting Summary Aggregation (Group By: {group_cols})...\")\n",
    "\n",
    "    # 1. Prepare for case-insensitive matching by uppercasing column names and group keys\n",
    "    \n",
    "    # Create a copy of the DF and standardize its column names to UPPERCASE\n",
    "    df_working = df.copy()\n",
    "    original_to_upper_map = {str(col): str(col).strip().upper() for col in df_working.columns}\n",
    "    df_working.columns = df_working.columns.map(original_to_upper_map)\n",
    "    \n",
    "    # Prepare the list of requested grouping columns in UPPERCASE\n",
    "    upper_group_cols = [col.upper().strip() for col in group_cols]\n",
    "    \n",
    "    # 2. Filter out missing grouping columns\n",
    "    present_group_cols_upper = [col for col in upper_group_cols if col in df_working.columns]\n",
    "    missing_cols_upper = [col for col in upper_group_cols if col not in df_working.columns]\n",
    "\n",
    "    if missing_cols_upper:\n",
    "        # Find the original mixed-case names for the missing columns to report to the user\n",
    "        missing_names = [group_cols[upper_group_cols.index(col)] for col in missing_cols_upper]\n",
    "        print(f\"âš ï¸ WARNING: Missing required grouping columns for aggregation: {missing_names}. Proceeding with aggregation on available columns only.\")\n",
    "    \n",
    "    if not present_group_cols_upper:\n",
    "        print(\"Error: No valid grouping columns found after filtering. Cannot aggregate.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    # 3. CRITICAL FIX: Explicitly drop 'GL ACCOUNT' if it's not a grouping key.\n",
    "    GL_ACCOUNT_COL = 'GL ACCOUNT'\n",
    "    if GL_ACCOUNT_COL in df_working.columns and GL_ACCOUNT_COL not in present_group_cols_upper:\n",
    "        print(f\" Â Â Note: Dropping non-grouping key column '{GL_ACCOUNT_COL}' before aggregation to prevent erroneous summation.\")\n",
    "        df_working = df_working.drop(columns=[GL_ACCOUNT_COL])\n",
    "    \n",
    "    # 4. Group and sum (pandas automatically sums only the numeric columns)\n",
    "    summary_df = df_working.groupby(present_group_cols_upper).sum().reset_index()\n",
    "\n",
    "    # 5. Rename columns back to the mixed-case standard for readability\n",
    "    \n",
    "    # Create the rename mapping from UPPERCASE (used for groupby) to MixedCase (desired output)\n",
    "    rename_dict = {}\n",
    "    used_mixed_case_cols = []\n",
    "    \n",
    "    for upper_col in present_group_cols_upper:\n",
    "        # Find the original mixed-case name from the requested group_cols list\n",
    "        index = upper_group_cols.index(upper_col)\n",
    "        mixed_case_name = group_cols[index]\n",
    "        rename_dict[upper_col] = mixed_case_name\n",
    "        used_mixed_case_cols.append(mixed_case_name)\n",
    "\n",
    "    summary_df = summary_df.rename(columns=rename_dict)\n",
    "    \n",
    "    # 6. Sort the final result by the grouping hierarchy (using only the columns present)\n",
    "    try:\n",
    "        summary_df = summary_df.sort_values(by=used_mixed_case_cols).reset_index(drop=True)\n",
    "    except KeyError:\n",
    "        print(\"Warning: Could not sort by specified group columns. Sorting skipped.\")\n",
    "\n",
    "    print(f\"âœ“ Summary table created with {len(summary_df)} aggregated rows. Grouped by: {used_mixed_case_cols}\")\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f81c91",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- STAGE 5: AGGREGATE TO FINAL SUMMARY (Current User Query) ---\n",
    "tb_fs_df = calculate_summary_totals(\n",
    "    df=trial_balance_pivot_table,\n",
    "    group_cols=[ 'TB Account Name', 'Account Type', 'FS Classification']\n",
    ")\n",
    "tb_fs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6e113b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "latest_reporting_dates_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298097d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_segmented_dfs(tb_fs_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Creates a dictionary of DataFrames, segmenting the input DataFrame\n",
    "    based on its numerical columns and dropping records where the metric value is zero.\n",
    "\n",
    "    Each resulting DataFrame contains the three identifying columns \n",
    "    ('TB Account Name', 'Account Type', 'FS Classification') as separate\n",
    "    data columns, followed by a single numerical column.\n",
    "    \n",
    "    CRITICAL CHANGE: Records with a 0 value in the numerical column are dropped.\n",
    "\n",
    "    Args:\n",
    "        tb_fs_df: The input DataFrame containing all financial and identifying data.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where keys are numerical column names and values are\n",
    "        the corresponding segmented DataFrames.\n",
    "    \"\"\"\n",
    "    # 1. Define the identifying columns to retain\n",
    "    IDENTIFIER_COLS: List[str] = ['TB Account Name', 'Account Type', 'FS Classification']\n",
    "\n",
    "    # 2. Identify the numerical columns based on data type\n",
    "    numerical_cols: List[str] = tb_fs_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "    # Filter out the identifier columns and known non-financial metrics\n",
    "    numerical_cols = [col for col in numerical_cols if col not in IDENTIFIER_COLS]\n",
    "    metrics_to_exclude = ['total_in_out_freq', 'total_out_in_freq', 'total_in_out_duration_hours', 'total_out_in_duration_hours']\n",
    "    numerical_cols = [col for col in numerical_cols if col not in metrics_to_exclude]\n",
    "\n",
    "    if not numerical_cols:\n",
    "        print(\"Warning: No numerical columns found to segment after exclusion filtering.\")\n",
    "        return {}\n",
    "\n",
    "    # 3. Create the dictionary of DataFrames\n",
    "    segmented_dfs: Dict[str, pd.DataFrame] = {}\n",
    "    \n",
    "    for num_col in numerical_cols:\n",
    "        # The list of columns for the new DataFrame\n",
    "        cols_to_select = IDENTIFIER_COLS + [num_col]\n",
    "\n",
    "        # Select the columns and create a deep copy\n",
    "        temp_df = tb_fs_df[cols_to_select].copy()\n",
    "        \n",
    "        # 4. CRITICAL STEP: Drop rows where the value in the current numerical column is exactly 0.\n",
    "        # Note: We use .loc to ensure we modify the copy safely.\n",
    "        temp_df = temp_df.loc[temp_df[num_col] != 0]\n",
    "\n",
    "        # Store the filtered DataFrame\n",
    "        segmented_dfs[num_col] = temp_df\n",
    "\n",
    "    return segmented_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ce016",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "segmented_results = create_segmented_dfs(tb_fs_df)\n",
    "print(segmented_results.keys())\n",
    "\n",
    "# segmented_results['PEMI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478eef9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "portfolio_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13bc8a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def export_financial_report(\n",
    "    dataframes: Dict[str, pd.DataFrame], \n",
    "    segmented_dfs: Dict[str, pd.DataFrame], \n",
    "    latest_reporting_dates: Dict[str, str],\n",
    "    portfolio_mapping: pd.DataFrame,\n",
    "    # UPDATED FILENAME HERE\n",
    "    output_filename: str = 'Trial_Balance.xlsx' \n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Exports multiple DataFrames and the dictionary of segmented DataFrames \n",
    "    to a single Excel file with specified sheet names and custom headers,\n",
    "    saving to a dynamic, date-based directory structure (../data/processed/Trail Balance/YYYY/).\n",
    "\n",
    "    Args:\n",
    "        dataframes: Dictionary of the initial 5 DataFrames.\n",
    "        segmented_dfs: Dictionary of DataFrames created by create_segmented_dfs.\n",
    "        latest_reporting_dates: Dictionary containing the reporting date information.\n",
    "        portfolio_mapping: DataFrame for fund name lookups.\n",
    "        output_filename: Name of the Excel file to create.\n",
    "\n",
    "    Returns:\n",
    "        The path to the generated Excel file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # DEBUG: Print input validation\n",
    "    print(f\"DEBUG: export_financial_report called\")\n",
    "    print(f\"DEBUG: dataframes keys: {list(dataframes.keys())}\")\n",
    "    print(f\"DEBUG: segmented_dfs keys: {list(segmented_dfs.keys())}\")\n",
    "    print(f\"DEBUG: output_filename: {output_filename}\")\n",
    "    \n",
    "    # 1. Prepare data and metadata\n",
    "    \n",
    "    # Define the sheet mapping for the initial 5 DFs\n",
    "    initial_sheet_map = {\n",
    "        'trial_balance_consolidated': 'RD-TB',\n",
    "        'chart_of_accounts_final': 'COA',\n",
    "        'coa_mapping': 'COA_ref',\n",
    "        'trial_balance_pivot_table': 'TB-Pivot',\n",
    "        'tb_fs_df': 'TB-SF',\n",
    "    }\n",
    "    \n",
    "    # Extract and format the reporting date\n",
    "    reporting_date_str = list(latest_reporting_dates.values())[0]\n",
    "    reporting_date = datetime.strptime(reporting_date_str, '%Y-%m-%d')\n",
    "    date_formatted = f\"as of {reporting_date.strftime('%B %d, %Y')}\" \n",
    "\n",
    "    # Dynamic Path Creation (IMPLEMENTATION OF USER REQUEST)\n",
    "    year = reporting_date.strftime('%Y')\n",
    "    \n",
    "    # Base directory now includes the fixed 'Trail Balance' subdirectory\n",
    "    base_dir = os.path.join('..', 'data', 'processed', 'Trail Balance')\n",
    "    \n",
    "    # Output directory now only contains the year, omitting the month\n",
    "    output_dir = os.path.join(base_dir, year)\n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    # Create the directory structure if it doesn't exist\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Directory created/verified: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating directory structure {output_dir}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"Export failed due to directory creation error.\"\n",
    "\n",
    "    # --- CRITICAL FIX: Clean up column names by stripping whitespace ---\n",
    "    portfolio_mapping.columns = portfolio_mapping.columns.str.strip()\n",
    "\n",
    "    # --- FIX: Check and normalize the 'level1accountname' column casing ---\n",
    "    target_col_name = 'level1accountname'\n",
    "    found_col = None\n",
    "    \n",
    "    # Iterate through columns to find the case-insensitive match\n",
    "    for col in portfolio_mapping.columns:\n",
    "        # Check if the stripped column name, converted to lowercase, matches the target\n",
    "        if col.lower() == target_col_name:\n",
    "            found_col = col\n",
    "            break\n",
    "            \n",
    "    if found_col is None:\n",
    "        # If the column is still not found, raise a clear error indicating what columns were present\n",
    "        raise KeyError(\n",
    "            f\"The required column '{target_col_name}' (case-insensitive) was not found in 'portfolio_mapping' DataFrame. \"\n",
    "            f\"Available columns are: {list(portfolio_mapping.columns)}\"\n",
    "        )\n",
    "        \n",
    "    # Standardize portfolio mapping level1accountname (convert to title case for cleaner output)\n",
    "    # Use the found column name for successful access\n",
    "    portfolio_mapping['level1accountname_std'] = portfolio_mapping[found_col].str.title()\n",
    "    portfolio_mapping_dict = portfolio_mapping.set_index('Fund_Code')['level1accountname_std'].to_dict()\n",
    "\n",
    "    # Define the starting row index for the DataFrame's column names (Excel Row 5).\n",
    "    # This pushes the data to start on Excel Row 6, inserting a blank row (Row 4) \n",
    "    # between the date header (Row 3) and the column names (Row 5).\n",
    "    HEADER_ROW_EXCEL_NUM = 5 \n",
    "    \n",
    "    # Define the standard number format: Thousand separator and 2 decimal places\n",
    "    NUMBER_FORMAT = '#,##0.00' \n",
    "    TOTAL_COLUMN_EXCEL_INDEX = 4 # The numerical value is in the 4th column (Excel column index 4)\n",
    "    # Define the bold font style\n",
    "    BOLD_FONT = Font(bold=True)\n",
    "\n",
    "    # 2. Initialize Excel Writer with openpyxl engine\n",
    "    try:\n",
    "        print(f\"Creating Excel file: {output_filepath}\")\n",
    "        # Use openpyxl engine and ensure the workbook object is accessible\n",
    "        writer = pd.ExcelWriter(output_filepath, engine='openpyxl')\n",
    "        \n",
    "        # 3. Write the initial 5 DataFrames\n",
    "        print(f\"Writing initial 5 DataFrames...\")\n",
    "        for df_key, sheet_name in initial_sheet_map.items():\n",
    "            if df_key in dataframes:\n",
    "                print(f\"  Writing sheet: {sheet_name} ({len(dataframes[df_key])} rows)\")\n",
    "                dataframes[df_key].to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                # Note: Default formatting is not applied to these sheets as they are raw dumps\n",
    "            else:\n",
    "                print(f\"âš ï¸ Warning: DataFrame '{df_key}' not found.\")\n",
    "                \n",
    "        # 4. Write the segmented DataFrames with custom headers and formatting\n",
    "        print(f\"Writing {len(segmented_dfs)} segmented DataFrames...\")\n",
    "        for num_col, df in segmented_dfs.items():\n",
    "            print(f\"  Writing sheet: {num_col} ({len(df)} rows)\")\n",
    "            sheet_name = num_col \n",
    "            \n",
    "            # Look up the fund name. Use the dictionary key (num_col) for lookup.\n",
    "            fund_name = portfolio_mapping_dict.get(\n",
    "                num_col, \n",
    "                \"Portfolio: Not Mapped\"\n",
    "            )\n",
    "            \n",
    "            # The list of header values for the first column (Rows 1, 2, 3)\n",
    "            custom_header = [\n",
    "                fund_name,             # Row 1, Col A: Level 1 Account Name\n",
    "                \"Trial Balance\",       # Row 2, Col A: Fixed string\n",
    "                date_formatted         # Row 3, Col A: As of Date\n",
    "            ]\n",
    "\n",
    "            # Write the DataFrame starting at the correct row index (Row 6)\n",
    "            df.to_excel(writer, sheet_name=sheet_name, startrow=HEADER_ROW_EXCEL_NUM, header=False, index=False)\n",
    "            \n",
    "            # Access the openpyxl workbook and worksheet to write the custom headers\n",
    "            worksheet = writer.sheets[sheet_name]\n",
    "            \n",
    "            # --- NEW IMPLEMENTATION: Merge and Bold Header Rows (1, 2, 3) ---\n",
    "            for row_index, value in enumerate(custom_header):\n",
    "                row_num_excel = row_index + 1\n",
    "                \n",
    "                # 1. Write the custom header value to the first cell (A1, A2, A3)\n",
    "                cell = worksheet.cell(row=row_num_excel, column=1, value=value)\n",
    "                \n",
    "                # 2. Apply bold font\n",
    "                cell.font = BOLD_FONT\n",
    "                \n",
    "                # 3. Merge cells A and B (columns 1 and 2) for a clean header\n",
    "                worksheet.merge_cells(start_row=row_num_excel, start_column=1, end_row=row_num_excel, end_column=2)\n",
    "            # --- END NEW HEADER IMPLEMENTATION ---\n",
    "                \n",
    "            # Write the DataFrame header (column names) above the data to Row 5\n",
    "            for col_num, column_name in enumerate(df.columns):\n",
    "                worksheet.cell(row=HEADER_ROW_EXCEL_NUM, column=col_num + 1, value=column_name)\n",
    "                \n",
    "\n",
    "            # --- NEW: Apply 2-decimal formatting to the numerical column (Column 4) ---\n",
    "            # Calculate data row boundaries\n",
    "            data_start_row_excel = HEADER_ROW_EXCEL_NUM + 1 # i.e., Excel row 6\n",
    "            data_end_row_excel = data_start_row_excel + len(df)\n",
    "            \n",
    "            for row_num in range(data_start_row_excel, data_end_row_excel):\n",
    "                cell = worksheet.cell(row=row_num, column=TOTAL_COLUMN_EXCEL_INDEX)\n",
    "                # Setting the number format for the data cells\n",
    "                cell.number_format = NUMBER_FORMAT\n",
    "\n",
    "            # --- NEW: Grand Total calculation and writing (skip 2 rows) ---\n",
    "            if not df.empty:\n",
    "                numerical_col_name = df.columns[-1]\n",
    "                # Safely calculate grand total\n",
    "                grand_total = pd.to_numeric(df[numerical_col_name], errors='coerce').fillna(0).sum()\n",
    "                \n",
    "                # Calculate the Excel row number for the total (Last data row + 2)\n",
    "                total_row_num = data_end_row_excel + 2\n",
    "                \n",
    "                # 1. Write the 'Grand Total' label in Column C (Column index 3)\n",
    "                label_cell = worksheet.cell(row=total_row_num, column=TOTAL_COLUMN_EXCEL_INDEX - 1, value=\"Grand Total\")\n",
    "                \n",
    "                # 2. Apply Right Alignment to the label cell\n",
    "                label_cell.alignment = Alignment(horizontal='right')\n",
    "                # 3. Make the label bold\n",
    "                label_cell.font = Font(bold=True)\n",
    "                \n",
    "                # Write the calculated sum in the numerical column (Column index 4) and APPLY FORMAT\n",
    "                total_cell = worksheet.cell(row=total_row_num, column=TOTAL_COLUMN_EXCEL_INDEX, value=grand_total)\n",
    "                total_cell.number_format = NUMBER_FORMAT # Apply the 2-decimal format\n",
    "            # --- END NEW IMPLEMENTATION ---\n",
    "\n",
    "        # 5. Save the file\n",
    "        print(f\"Saving Excel file...\")\n",
    "        writer.close()\n",
    "        print(f\"âœ… Excel file saved successfully: {output_filepath}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR: An error occurred during Excel export: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"Export failed.\"\n",
    "\n",
    "    return output_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af16325",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "portfolio_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6ee703",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 3. Prepare the dictionary of the 5 main DFs explicitly from the unpacked variables\n",
    "# This dictionary matches the expected input structure for the export function.\n",
    "main_dfs = {\n",
    "    'trial_balance_consolidated': trial_balance_consolidated, \n",
    "    'chart_of_accounts_final': chart_of_accounts_final, \n",
    "    'coa_mapping': coa_mapping, \n",
    "    'trial_balance_pivot_table': trial_balance_pivot_table, \n",
    "    'tb_fs_df': tb_fs_df\n",
    "}\n",
    "\n",
    "# 4. Export the final report\n",
    "output_file = export_financial_report(\n",
    "    dataframes=main_dfs,\n",
    "    segmented_dfs=segmented_results,\n",
    "    latest_reporting_dates=latest_reporting_dates_flat,\n",
    "    portfolio_mapping=portfolio_mapping,\n",
    "    # Passing the new desired filename explicitly\n",
    "    output_filename='Trial_Balance.xlsx' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5c3d0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tb_fs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195acb27",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _get_metric_label_from_key(metric_key: str) -> str:\n",
    "    \"\"\"\n",
    "    Parses a metric key like '2023_Q1_Actual' to return a shorter label like 'Q1 Actual'.\n",
    "    \"\"\"\n",
    "    parts = metric_key.split('_')\n",
    "    if len(parts) >= 3:\n",
    "        # Assumes format YYYY_Period_Type (e.g., 2023_Q1_Actual)\n",
    "        period = parts[-2] \n",
    "        metric_type = parts[-1] \n",
    "        return f\"{period} {metric_type}\"\n",
    "    return metric_key # Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fef12ce",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def export_segmented_summary(\n",
    "    segmented_dfs: Dict[str, pd.DataFrame], \n",
    "    latest_reporting_dates: Dict[str, str],\n",
    "    portfolio_mapping: pd.DataFrame,\n",
    "    output_filename: str = 'Segmented_Summary.xlsx' \n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Exports segmented DataFrames to a single Excel file, applying the user's requested\n",
    "    custom header structure, grand totals, and 2-decimal formatting.\n",
    "\n",
    "    The data insertion is adjusted to start after 5 rows (on Excel Row 6).\n",
    "    \"\"\"\n",
    "    \n",
    "    # DEBUG: Print input data\n",
    "    print(f\"DEBUG: segmented_dfs keys: {list(segmented_dfs.keys()) if segmented_dfs else 'EMPTY'}\")\n",
    "    print(f\"DEBUG: latest_reporting_dates: {latest_reporting_dates}\")\n",
    "    print(f\"DEBUG: output_filename: {output_filename}\")\n",
    "    \n",
    "    # 1. Prepare metadata\n",
    "    \n",
    "    # Extract month name used as the key in the reporting dates dict\n",
    "    month_name_key = list(latest_reporting_dates.keys())[0]\n",
    "\n",
    "    # Extract and format the reporting date\n",
    "    reporting_date_str = list(latest_reporting_dates.values())[0]\n",
    "    reporting_date = datetime.strptime(reporting_date_str, '%Y-%m-%d')\n",
    "    date_formatted = f\"as of {reporting_date.strftime('%B %d, %Y')}\" \n",
    "\n",
    "    # Dynamic Path Creation (Uses YYYY only in directory structure)\n",
    "    year = reporting_date.strftime('%Y')\n",
    "    \n",
    "    # The new base directory should include the 'Trail Balance' subdirectory\n",
    "    base_dir = os.path.join('..', 'data', 'processed', 'Trail Balance') \n",
    "    \n",
    "    # The output directory now only contains the year, skipping the month folder\n",
    "    output_dir = os.path.join(base_dir, year)\n",
    "    \n",
    "    output_filepath = os.path.join(output_dir, output_filename)\n",
    "    \n",
    "    # Create the directory structure if it doesn't exist\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Directory created/verified: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating directory structure {output_dir}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"Export failed due to directory creation error.\"\n",
    "    \n",
    "    # Clean up column names in portfolio_mapping for robust lookup\n",
    "    portfolio_mapping.columns = portfolio_mapping.columns.str.strip()\n",
    "    target_col_name = 'level1accountname'\n",
    "    found_col = None\n",
    "    for col in portfolio_mapping.columns:\n",
    "        if col.lower() == target_col_name:\n",
    "            found_col = col\n",
    "            break\n",
    "            \n",
    "    if found_col is None:\n",
    "        raise KeyError(\n",
    "            f\"Required column '{target_col_name}' was not found in 'portfolio_mapping'.\"\n",
    "        )\n",
    "        \n",
    "    portfolio_mapping['level1accountname_std'] = portfolio_mapping[found_col].str.title()\n",
    "    portfolio_mapping_dict = portfolio_mapping.set_index('Fund_Code')['level1accountname_std'].to_dict()\n",
    "\n",
    "    # Define the starting row index for the DataFrame's column names (Excel Row 5).\n",
    "    HEADER_ROW_EXCEL_NUM = 5 \n",
    "    DATA_START_ROW = HEADER_ROW_EXCEL_NUM + 1 # For df.to_excel startrow index\n",
    "\n",
    "    # Define the standard number format and column index for numerical data\n",
    "    NUMBER_FORMAT = '#,##0.00' \n",
    "    TOTAL_COLUMN_EXCEL_INDEX = 4 # The numerical value is in the 4th column (Excel column index 4)\n",
    "    # Define the bold font style\n",
    "    BOLD_FONT = Font(bold=True)\n",
    "\n",
    "    # 2. Initialize Excel Writer with openpyxl engine\n",
    "    try:\n",
    "        print(f\"Creating Excel file at: {output_filepath}\")\n",
    "        writer = pd.ExcelWriter(output_filepath, engine='openpyxl')\n",
    "        \n",
    "        # 3. Write the segmented DataFrames with custom headers\n",
    "        for num_col, df in segmented_dfs.items():\n",
    "            print(f\"  Writing sheet: {num_col} ({len(df)} rows)\")\n",
    "            sheet_name = num_col \n",
    "            \n",
    "            # Look up the fund name\n",
    "            fund_name = portfolio_mapping_dict.get(\n",
    "                num_col, \n",
    "                f\"Portfolio: {num_col}\"\n",
    "            )\n",
    "            \n",
    "            # The list of header values for the first column (Rows 1, 2, 3)\n",
    "            custom_header = [\n",
    "                fund_name,             # Row 1, Col A\n",
    "                \"Trial Balance Summary\", # Row 2, Col A\n",
    "                date_formatted         # Row 3, Col A\n",
    "            ]\n",
    "\n",
    "            # Use the month name as the 4th column header\n",
    "            value_col_header = month_name_key \n",
    "\n",
    "            # Write the DataFrame data (without headers) starting at Row 6\n",
    "            df.to_excel(writer, sheet_name=sheet_name, startrow=DATA_START_ROW, header=False, index=False)\n",
    "            \n",
    "            # Access the openpyxl workbook and worksheet to write the custom headers\n",
    "            worksheet = writer.sheets[sheet_name]\n",
    "            \n",
    "            # --- NEW IMPLEMENTATION: Merge and Bold Header Rows (1, 2, 3) ---\n",
    "            for row_index, value in enumerate(custom_header):\n",
    "                row_num_excel = row_index + 1\n",
    "                \n",
    "                # 1. Write the custom header value to the first cell (A1, A2, A3)\n",
    "                cell = worksheet.cell(row=row_num_excel, column=1, value=value)\n",
    "                \n",
    "                # 2. Apply bold font\n",
    "                cell.font = BOLD_FONT\n",
    "                \n",
    "                # 3. Merge cells A and B (columns 1 and 2) for a clean header\n",
    "                worksheet.merge_cells(start_row=row_num_excel, start_column=1, end_row=row_num_excel, end_column=2)\n",
    "            # --- END NEW HEADER IMPLEMENTATION ---\n",
    "            \n",
    "            # # Write the custom headers to Column A (column index 1)\n",
    "            # for row_index, value in enumerate(custom_header):\n",
    "            #     # Excel rows are 1-based, custom_header elements go into rows 1, 2, 3\n",
    "            #     worksheet.cell(row=row_index + 1, column=1, value=value)\n",
    "                \n",
    "            # Write the DataFrame header (column names) to Row 5 (index 4)\n",
    "            # The identifiers: 'TB Account Name', 'Account Type', 'FS Classification'\n",
    "            for col_num, column_name in enumerate(df.columns[:3]):\n",
    "                worksheet.cell(row=HEADER_ROW_EXCEL_NUM, column=col_num + 1, value=column_name)\n",
    "\n",
    "            # Write the custom metric header (e.g., 'September') to Row 5, Column 4\n",
    "            worksheet.cell(\n",
    "                row=HEADER_ROW_EXCEL_NUM, \n",
    "                column=4, \n",
    "                value=value_col_header\n",
    "            )\n",
    "            \n",
    "            # --- START NEW IMPLEMENTATION: Formatting and Grand Total ---\n",
    "            \n",
    "            # 1. Apply 2-decimal formatting to the numerical column (Column 4)\n",
    "            \n",
    "            # Calculate data row boundaries\n",
    "            data_start_row_excel = DATA_START_ROW # i.e., Excel row 6\n",
    "            data_end_row_excel = data_start_row_excel + len(df)\n",
    "            \n",
    "            for row_num in range(data_start_row_excel, data_end_row_excel):\n",
    "                cell = worksheet.cell(row=row_num, column=TOTAL_COLUMN_EXCEL_INDEX)\n",
    "                # Setting the number format for the data cells\n",
    "                cell.number_format = NUMBER_FORMAT\n",
    "\n",
    "            # 2. Grand Total calculation and writing (skip 2 rows)\n",
    "            if not df.empty:\n",
    "                numerical_col_name = df.columns[-1]\n",
    "                # Safely calculate grand total\n",
    "                grand_total = pd.to_numeric(df[numerical_col_name], errors='coerce').fillna(0).sum()\n",
    "                \n",
    "                # Calculate the Excel row number for the total (Last data row + 2)\n",
    "                total_row_num = data_end_row_excel + 2\n",
    "                \n",
    "                # 1. Write the 'Grand Total' label in Column C (Column index 3)\n",
    "                label_cell = worksheet.cell(row=total_row_num, column=TOTAL_COLUMN_EXCEL_INDEX - 1, value=\"Grand Total\")\n",
    "                \n",
    "                # 2. Apply Right Alignment to the label cell\n",
    "                label_cell.alignment = Alignment(horizontal='right')\n",
    "                # 3. Make the label bold\n",
    "                label_cell.font = Font(bold=True)\n",
    "                \n",
    "                # Write the 'Grand Total' label in Column A (Column index 1)\n",
    "                worksheet.cell(row=total_row_num, column=TOTAL_COLUMN_EXCEL_INDEX - 1, value=\"Grand Total\")\n",
    "                \n",
    "                # Write the calculated sum in the numerical column (Column index 4) and APPLY FORMAT\n",
    "                total_cell = worksheet.cell(row=total_row_num, column=TOTAL_COLUMN_EXCEL_INDEX, value=grand_total)\n",
    "                total_cell.number_format = NUMBER_FORMAT # Apply the 2-decimal format\n",
    "            # --- END NEW IMPLEMENTATION ---\n",
    "\n",
    "\n",
    "        # 4. Save the file\n",
    "        print(f\"Saving Excel file...\")\n",
    "        writer.close()\n",
    "        print(f\"âœ… Excel file saved successfully: {output_filepath}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR: An error occurred during Excel export: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return \"Export failed.\"\n",
    "\n",
    "    return output_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d952b75",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3. Export the new segmented summary report\n",
    "output_file = export_segmented_summary(\n",
    "    segmented_dfs=segmented_results,\n",
    "    latest_reporting_dates=latest_reporting_dates_flat,\n",
    "    portfolio_mapping=portfolio_mapping,\n",
    "    # UPDATED FILENAME HERE\n",
    "    output_filename='Trial Balance Monthly.xlsx' \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2bf5ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix file base on year folder -> for naming convention if on month data on the file use num val for identification i,e january = TB_2025_1, if january and feb = TB_2025_1-2, \n",
    "# if jan to jul = TB_{year}_1-6 = TB_{year}_{month_num_range} \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4bc09f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## 10. Automation Workflow - [Next Steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f408ac07",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Add automation logic here\n",
    "# - Validation\n",
    "# - Reconciliation\n",
    "# - Report generation\n",
    "# - Export processed data\n",
    "\n",
    "print(\"Ready for automation workflow implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8406c1f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "D:\\UserProfile\\Documents\\@ VFC\\pemi-automation\\trial-balance\\notebooks\\01-rd-trial-balance-mvp.ipynb",
   "output_path": "D:\\UserProfile\\Documents\\@ VFC\\pemi-automation\\trial-balance\\notebooks\\executed_trial_balance_reports\\trial_balance_report_20251121_155228.ipynb",
   "parameters": {},
   "start_time": "2025-11-21T07:52:30.254887",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}